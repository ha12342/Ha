{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "59a935dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\ctarg416\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ctarg416\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (2.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ctarg416\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\ctarg416\\appdata\\roaming\\python\\python310\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\ctarg416\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (0.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2266919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal #dùng cho môi trường liên tục\n",
    "from torch.distributions import Categorical #Lựa chọn hành động từ phân phối rời rạc.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0eaf5a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945d631",
   "metadata": {},
   "source": [
    "Tạo RolloutBuffer, lưu data lại để train, tạo hàm để update lúc train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "17837276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RolloutBuffer Bộ nhớ tạm lưu giữ thông tin huấn luyện sau\n",
    "#lưu trữ tạm thời các quỹ đạo \n",
    "#tập hợp dữ liệu về các tương tác của tác nhân (agent) với môi trường trong một tập hợp (episode), tính hàm lợi thế\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = [] #Lưu các hành động đã thực hiện\n",
    "        self.states = []  #Lưu các trạng thái hiện tại\n",
    "        self.logprobs = []#Lưu log của xác suất pi(a|s)\n",
    "        self.rewards = [] #Lưu lại reward\n",
    "        self.state_values = [] # Giá trị trạng thái V do critic dự đoán, để tính hàm lợi thế\n",
    "        self.is_terminals = [] #Lưu lại cờ kết thúc\n",
    "\n",
    "\n",
    "    def clear(self): #xóa buffer để update data mới\n",
    "        del self.actions[:] \n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "#Actor: Mạng chính sách, xuất ra phân phối xác suất dựa trên trạng thái\n",
    "#Critic: Mạng giá trị, Ước lượng giá trị trạng thái (Vt) để tính hàm ưu tiên\n",
    "#hàm này giúp Agent chọn lọc hành động, đánh giá được trạng thái.\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
    "        #state_dim: Số chiều của state\n",
    "        #action_dim: số chiều của action\n",
    "        #xem action có liên tục không\n",
    "        #action_std_init: độ lệch chuẩn cho phân phối liên tục\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        #Nếu hành động liên tục lưu lại chiều hành động chuyển đến phương sai action_std_init^2\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "\n",
    "        # ACTOR\n",
    "        if has_continuous_action_space : # continuous action space\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Tanh()\n",
    "        #Nếu là hành động liên tục thì đầu ra là trung bình u(s) phân phối Gauss => phân phối hành động π(a|s) = N(u,tổng)\n",
    "        #Tanh giới hạn đầu ra từ [-1,1], phù hợp vs phạm v hành động\n",
    "                        )\n",
    "        else: # discrete action space\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "        #Không gian rời rạc: xuất ra xác suất cho từng action rời rạc theo Softmax\n",
    "\n",
    "        # CRITIC\n",
    "        # Tương tự như actor nhưng xuất ra là 1 giá trị đơn ước lượng giá trị trạng thái Vt\n",
    "        # Dùng để tính hàm lợi thế At = Q(s,a) - Vt\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "        \n",
    "    #Cập nhật phương sai cho hành động liên tục\n",
    "    #tức là điều chỉnh khám phá, độ biến động cho các action\n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    \n",
    "    def act(self, state): #đầu vào là state hiện tại\n",
    "\n",
    "        # ACTOR\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state) #Nếu đầu vào là state liên tục thì đầu ra actor là giá trị trung bình u phân phối gauss\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0) #phương sai Cov\n",
    "            dist = MultivariateNormal(action_mean, cov_mat) #tạo phân phối đa biến\n",
    "        else:\n",
    "            action_probs = self.actor(state) #đầu ra softmax\n",
    "            dist = Categorical(action_probs) #phân phối rời rạc\n",
    "\n",
    "        action = dist.sample() #chọn hành động bằng cách lấy mẫu phân phối \n",
    "        action_logprob = dist.log_prob(action) #lấy log của các suất được chọn (π(a|s))\n",
    "\n",
    "        # CRITIC\n",
    "        state_val = self.critic(state) #xuất ra giá Vt từ critic\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach() #trả về action, log, Vt để lưu vào RolloutBuffer\n",
    "\n",
    "    #Dùng để tính toán lại các giá trị cần thiết khi cập nhật danh sách:\n",
    "        #+Log xác suất hành động \n",
    "        #+Entropy của chính sách\n",
    "        #+Ước lượng giá trị trạng thái Vt\n",
    "    # Đầu vào là state, action cũ đầu ra là logprob (mới), value (mới), entropy\n",
    "    # So sánh policy cũ và mới.\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        # ACTOR\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "            # for single action continuous environments\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "        # CRITIC\n",
    "        state_values = self.critic(state)\n",
    "\n",
    "        return action_logprobs, state_values, dist_entropy \n",
    "# Tại sao có act rồi cần dùng evaluate:\n",
    "#    + act dùng để action và thu thập dữ liệu xong lưu vào RolloutBuffer để train\n",
    "#    + evaluate dùng để tính toán train, policy cũ và mới: ratio, advantage, entropy.\n",
    "# act → buffer → evaluate → loss → update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982baa03",
   "metadata": {},
   "source": [
    "Tạo 1 class Train model để có được policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ed682dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "    # state_dim, action_dim số chiều của trạng thái và hành động\n",
    "    # lr_actor, lr_critic: tốc độ học cho actor và critic\n",
    "    # gamma: hệ số chiết khấu phần thưởng trong tương lai.\n",
    "    # K_epochs: số lần cập nhật chính sách cho mỗi batch.\n",
    "    # eps_clip: hệ số clip PPO để giới hạn thay đổi chính sách.\n",
    "    # action_std_init: độ lệch chuẩn ban đầu cho các hành động liên tục.\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.buffer = RolloutBuffer() # để lưu các data trong 1 batch\n",
    "\n",
    "\n",
    "        # Tạo 2 mạng:\n",
    "        #    + policy: Mạng chính sách đang được huấn luyện \n",
    "        #    + policy_old: Mạng chính sách cũ, dùng để thu thập dữ liệu, ban đầu sao chép từ policy. \n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    # điều chỉnh độ lệch chuẩn để điều chỉnh độ khám phá \n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    # giảm dần độ lệch chuẩn từ khám phá sang khai thác khi học đủ nhiều.\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    #Chọn hành động và lưu lại thông tin\n",
    "    def select_action(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device) #chuyển state thành tensor\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "                # Dùng policy_old để lấy action, action_logprob, state_val\n",
    "    \n",
    "            # Lưu lại vào RolloutBuffer\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten() #nếu không gian liên tục trả về action dưới dạng NumPy array.\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.item() #trả về action nếu không gian rời rạc\n",
    "\n",
    "    #Cập nhật policy dựa trên dữ liệu trên bộ đệm, cải thiện chính sách qua K_epochs\n",
    "    def update(self):\n",
    "\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = [] #để lưu lại tổng reward mỗi lần khi train\n",
    "        discounted_reward = 0 #để tính tống reward\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)): #lấy reward và is_terminals trong buffer\n",
    "            #duyệt ngược lại dưới lên\n",
    "            if is_terminal: \n",
    "                discounted_reward = 0 #nếu điểm cuỗi của 1 eposide thì phần thưởng về sau xóa\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward) #cộng phần thưởng hiện tại vs tương lai * gamma vì đi ngược dưới lên nên hợp lý\n",
    "            rewards.insert(0, discounted_reward) # lưu tổng reward lại\n",
    "\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device) #chuẩn hóa rewards thành tensor\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7) #đưa phần thưởng về 1 dạng chuẩn\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach() #tính toán hàm lợi thế\n",
    "\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs): \n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions) #để lấy hành động mới dưới chính sách hiện tại\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach()) #ratios\n",
    "\n",
    "\n",
    "\n",
    "            ## Xây dừng hàm loss clipped \n",
    "            # Finding Surrogate Loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy #tổng loss\n",
    "\n",
    "            # take gradient step xóa grad cũ của loss, tối ưu theo Adam\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict()) #chép từ policy sang policy_old\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "\n",
    "    #lưu policy_old vào tệp \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "\n",
    "    #tải từ tệp model vào policy và policy_old\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "\n",
    "\n",
    "\n",
    "## 1. Dùng select_action và policy_old để tương tác môi trường, lưu vào bộ đệm để train \n",
    "#  2. Sau khi buffer đầy thì dùng update để train policy\n",
    "#  3. Sao chép policy sang policy_old\n",
    "#  4. Xóa buffer, lặp lại từ 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b24666",
   "metadata": {},
   "source": [
    "Bắt đầu train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b6ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training environment name : CartPole-v1\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  300000\n",
      "max timesteps per episode :  1000\n",
      "log frequency : 2000 timesteps\n",
      "printing average reward over episodes in last : 4000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  4\n",
      "action space dimension :  2\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a discrete action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 4000 timesteps\n",
      "PPO K epochs :  40\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.0003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Episode  0 : Reward =  12.0\n",
      "Saving better model at episode 0 with reward 12.0\n",
      "Episode  1 : Reward =  24.0\n",
      "Saving better model at episode 1 with reward 24.0\n",
      "Episode  2 : Reward =  20.0\n",
      "Episode  3 : Reward =  17.0\n",
      "Episode  4 : Reward =  12.0\n",
      "Episode  5 : Reward =  9.0\n",
      "Episode  6 : Reward =  20.0\n",
      "Episode  7 : Reward =  18.0\n",
      "Episode  8 : Reward =  18.0\n",
      "Episode  9 : Reward =  36.0\n",
      "Saving better model at episode 9 with reward 36.0\n",
      "Episode  10 : Reward =  10.0\n",
      "Episode  11 : Reward =  13.0\n",
      "Episode  12 : Reward =  32.0\n",
      "Episode  13 : Reward =  13.0\n",
      "Episode  14 : Reward =  17.0\n",
      "Episode  15 : Reward =  33.0\n",
      "Episode  16 : Reward =  15.0\n",
      "Episode  17 : Reward =  35.0\n",
      "Episode  18 : Reward =  12.0\n",
      "Episode  19 : Reward =  13.0\n",
      "Episode  20 : Reward =  16.0\n",
      "Episode  21 : Reward =  10.0\n",
      "Episode  22 : Reward =  12.0\n",
      "Episode  23 : Reward =  16.0\n",
      "Episode  24 : Reward =  23.0\n",
      "Episode  25 : Reward =  20.0\n",
      "Episode  26 : Reward =  12.0\n",
      "Episode  27 : Reward =  18.0\n",
      "Episode  28 : Reward =  11.0\n",
      "Episode  29 : Reward =  18.0\n",
      "Episode  30 : Reward =  10.0\n",
      "Episode  31 : Reward =  12.0\n",
      "Episode  32 : Reward =  13.0\n",
      "Episode  33 : Reward =  61.0\n",
      "Saving better model at episode 33 with reward 61.0\n",
      "Episode  34 : Reward =  14.0\n",
      "Episode  35 : Reward =  35.0\n",
      "Episode  36 : Reward =  18.0\n",
      "Episode  37 : Reward =  37.0\n",
      "Episode  38 : Reward =  18.0\n",
      "Episode  39 : Reward =  44.0\n",
      "Episode  40 : Reward =  25.0\n",
      "Episode  41 : Reward =  22.0\n",
      "Episode  42 : Reward =  15.0\n",
      "Episode  43 : Reward =  18.0\n",
      "Episode  44 : Reward =  18.0\n",
      "Episode  45 : Reward =  15.0\n",
      "Episode  46 : Reward =  28.0\n",
      "Episode  47 : Reward =  17.0\n",
      "Episode  48 : Reward =  14.0\n",
      "Episode  49 : Reward =  37.0\n",
      "Episode  50 : Reward =  21.0\n",
      "Episode  51 : Reward =  45.0\n",
      "Episode  52 : Reward =  10.0\n",
      "Episode  53 : Reward =  53.0\n",
      "Episode  54 : Reward =  9.0\n",
      "Episode  55 : Reward =  11.0\n",
      "Episode  56 : Reward =  15.0\n",
      "Episode  57 : Reward =  24.0\n",
      "Episode  58 : Reward =  9.0\n",
      "Episode  59 : Reward =  21.0\n",
      "Episode  60 : Reward =  14.0\n",
      "Episode  61 : Reward =  12.0\n",
      "Episode  62 : Reward =  36.0\n",
      "Episode  63 : Reward =  14.0\n",
      "Episode  64 : Reward =  38.0\n",
      "Episode  65 : Reward =  13.0\n",
      "Episode  66 : Reward =  10.0\n",
      "Episode  67 : Reward =  10.0\n",
      "Episode  68 : Reward =  12.0\n",
      "Episode  69 : Reward =  10.0\n",
      "Episode  70 : Reward =  13.0\n",
      "Episode  71 : Reward =  54.0\n",
      "Episode  72 : Reward =  14.0\n",
      "Episode  73 : Reward =  30.0\n",
      "Episode  74 : Reward =  26.0\n",
      "Episode  75 : Reward =  15.0\n",
      "Episode  76 : Reward =  16.0\n",
      "Episode  77 : Reward =  29.0\n",
      "Episode  78 : Reward =  24.0\n",
      "Episode  79 : Reward =  15.0\n",
      "Episode  80 : Reward =  12.0\n",
      "Episode  81 : Reward =  20.0\n",
      "Episode  82 : Reward =  17.0\n",
      "Episode  83 : Reward =  13.0\n",
      "Episode  84 : Reward =  23.0\n",
      "Episode  85 : Reward =  14.0\n",
      "Episode  86 : Reward =  14.0\n",
      "Episode  87 : Reward =  22.0\n",
      "Episode  88 : Reward =  19.0\n",
      "Episode  89 : Reward =  19.0\n",
      "Episode  90 : Reward =  32.0\n",
      "Episode  91 : Reward =  16.0\n",
      "Episode  92 : Reward =  11.0\n",
      "Episode  93 : Reward =  15.0\n",
      "Episode  94 : Reward =  19.0\n",
      "Episode  95 : Reward =  13.0\n",
      "Episode  96 : Reward =  15.0\n",
      "Episode  97 : Reward =  10.0\n",
      "Episode  98 : Reward =  27.0\n",
      "Episode  99 : Reward =  10.0\n",
      "Episode  100 : Reward =  27.0\n",
      "Episode  101 : Reward =  20.0\n",
      "Episode  102 : Reward =  16.0\n",
      "Episode  103 : Reward =  39.0\n",
      "Episode  104 : Reward =  22.0\n",
      "Episode  105 : Reward =  14.0\n",
      "Episode  106 : Reward =  10.0\n",
      "Episode  107 : Reward =  15.0\n",
      "Episode  108 : Reward =  16.0\n",
      "Episode  109 : Reward =  16.0\n",
      "Episode  110 : Reward =  14.0\n",
      "Episode  111 : Reward =  12.0\n",
      "Episode  112 : Reward =  14.0\n",
      "Episode  113 : Reward =  25.0\n",
      "Episode  114 : Reward =  27.0\n",
      "Episode  115 : Reward =  14.0\n",
      "Episode  116 : Reward =  22.0\n",
      "Episode  117 : Reward =  14.0\n",
      "Episode  118 : Reward =  16.0\n",
      "Episode  119 : Reward =  41.0\n",
      "Episode  120 : Reward =  19.0\n",
      "Episode  121 : Reward =  17.0\n",
      "Episode  122 : Reward =  24.0\n",
      "Episode  123 : Reward =  20.0\n",
      "Episode  124 : Reward =  11.0\n",
      "Episode  125 : Reward =  8.0\n",
      "Episode  126 : Reward =  15.0\n",
      "Episode  127 : Reward =  18.0\n",
      "Episode  128 : Reward =  13.0\n",
      "Episode  129 : Reward =  16.0\n",
      "Episode  130 : Reward =  12.0\n",
      "Episode  131 : Reward =  49.0\n",
      "Episode  132 : Reward =  15.0\n",
      "Episode  133 : Reward =  15.0\n",
      "Episode  134 : Reward =  21.0\n",
      "Episode  135 : Reward =  13.0\n",
      "Episode  136 : Reward =  33.0\n",
      "Episode  137 : Reward =  11.0\n",
      "Episode  138 : Reward =  16.0\n",
      "Episode  139 : Reward =  17.0\n",
      "Episode  140 : Reward =  21.0\n",
      "Episode  141 : Reward =  20.0\n",
      "Episode  142 : Reward =  18.0\n",
      "Episode  143 : Reward =  22.0\n",
      "Episode  144 : Reward =  20.0\n",
      "Episode  145 : Reward =  21.0\n",
      "Episode  146 : Reward =  21.0\n",
      "Episode  147 : Reward =  26.0\n",
      "Episode  148 : Reward =  14.0\n",
      "Episode  149 : Reward =  13.0\n",
      "Episode  150 : Reward =  13.0\n",
      "Episode  151 : Reward =  25.0\n",
      "Episode  152 : Reward =  10.0\n",
      "Episode  153 : Reward =  54.0\n",
      "Episode  154 : Reward =  17.0\n",
      "Episode  155 : Reward =  47.0\n",
      "Episode  156 : Reward =  9.0\n",
      "Episode  157 : Reward =  54.0\n",
      "Episode  158 : Reward =  15.0\n",
      "Episode  159 : Reward =  22.0\n",
      "Episode  160 : Reward =  21.0\n",
      "Episode  161 : Reward =  48.0\n",
      "Episode  162 : Reward =  14.0\n",
      "Episode  163 : Reward =  8.0\n",
      "Episode  164 : Reward =  36.0\n",
      "Episode  165 : Reward =  22.0\n",
      "Episode  166 : Reward =  20.0\n",
      "Episode  167 : Reward =  14.0\n",
      "Episode  168 : Reward =  11.0\n",
      "Episode  169 : Reward =  19.0\n",
      "Episode  170 : Reward =  10.0\n",
      "Episode  171 : Reward =  15.0\n",
      "Episode  172 : Reward =  15.0\n",
      "Episode  173 : Reward =  24.0\n",
      "Episode  174 : Reward =  19.0\n",
      "Episode  175 : Reward =  28.0\n",
      "Episode  176 : Reward =  23.0\n",
      "Episode  177 : Reward =  15.0\n",
      "Episode  178 : Reward =  15.0\n",
      "Episode  179 : Reward =  21.0\n",
      "Episode  180 : Reward =  21.0\n",
      "Episode  181 : Reward =  21.0\n",
      "Episode  182 : Reward =  13.0\n",
      "Episode  183 : Reward =  21.0\n",
      "Episode  184 : Reward =  20.0\n",
      "Episode  185 : Reward =  32.0\n",
      "Episode  186 : Reward =  15.0\n",
      "Episode  187 : Reward =  22.0\n",
      "Episode  188 : Reward =  11.0\n",
      "Episode  189 : Reward =  12.0\n",
      "Episode  190 : Reward =  16.0\n",
      "Episode  191 : Reward =  22.0\n",
      "Episode  192 : Reward =  12.0\n",
      "Episode  193 : Reward =  15.0\n",
      "Episode  194 : Reward =  20.0\n",
      "Episode  195 : Reward =  11.0\n",
      "Episode  196 : Reward =  15.0\n",
      "Episode  197 : Reward =  28.0\n",
      "Episode  198 : Reward =  11.0\n",
      "Episode  199 : Reward =  23.0\n",
      "Episode  200 : Reward =  30.0\n",
      "Episode  201 : Reward =  13.0\n",
      "Episode  202 : Reward =  72.0\n",
      "Saving better model at episode 202 with reward 72.0\n",
      "Episode  203 : Reward =  20.0\n",
      "Episode  204 : Reward =  20.0\n",
      "Episode  205 : Reward =  13.0\n",
      "Episode  206 : Reward =  25.0\n",
      "Episode  207 : Reward =  14.0\n",
      "Episode  208 : Reward =  15.0\n",
      "Episode  209 : Reward =  23.0\n",
      "Episode  210 : Reward =  44.0\n",
      "Episode  211 : Reward =  28.0\n",
      "Episode  212 : Reward =  16.0\n",
      "Episode  213 : Reward =  20.0\n",
      "Episode  214 : Reward =  13.0\n",
      "Episode  215 : Reward =  41.0\n",
      "Episode  216 : Reward =  12.0\n",
      "Episode  217 : Reward =  10.0\n",
      "Episode  218 : Reward =  26.0\n",
      "Episode  219 : Reward =  22.0\n",
      "Episode  220 : Reward =  14.0\n",
      "Episode  221 : Reward =  49.0\n",
      "Episode  222 : Reward =  14.0\n",
      "Episode  223 : Reward =  14.0\n",
      "Episode  224 : Reward =  31.0\n",
      "Episode  225 : Reward =  14.0\n",
      "Episode  226 : Reward =  27.0\n",
      "Episode  227 : Reward =  11.0\n",
      "Episode  228 : Reward =  20.0\n",
      "Episode  229 : Reward =  21.0\n",
      "Episode  230 : Reward =  22.0\n",
      "Episode  231 : Reward =  12.0\n",
      "Episode  232 : Reward =  21.0\n",
      "Episode  233 : Reward =  24.0\n",
      "Episode  234 : Reward =  20.0\n",
      "Episode  235 : Reward =  13.0\n",
      "Episode  236 : Reward =  14.0\n",
      "Episode  237 : Reward =  8.0\n",
      "Episode  238 : Reward =  19.0\n",
      "Episode  239 : Reward =  39.0\n",
      "Episode  240 : Reward =  24.0\n",
      "Episode  241 : Reward =  16.0\n",
      "Episode  242 : Reward =  11.0\n",
      "Episode  243 : Reward =  42.0\n",
      "Episode  244 : Reward =  12.0\n",
      "Episode  245 : Reward =  16.0\n",
      "Episode  246 : Reward =  25.0\n",
      "Episode  247 : Reward =  14.0\n",
      "Episode  248 : Reward =  22.0\n",
      "Episode  249 : Reward =  20.0\n",
      "Episode  250 : Reward =  13.0\n",
      "Episode  251 : Reward =  37.0\n",
      "Episode  252 : Reward =  33.0\n",
      "Episode  253 : Reward =  33.0\n",
      "Episode  254 : Reward =  17.0\n",
      "Episode  255 : Reward =  22.0\n",
      "Episode  256 : Reward =  30.0\n",
      "Episode  257 : Reward =  12.0\n",
      "Episode  258 : Reward =  18.0\n",
      "Episode  259 : Reward =  57.0\n",
      "Episode  260 : Reward =  20.0\n",
      "Episode  261 : Reward =  13.0\n",
      "Episode  262 : Reward =  12.0\n",
      "Episode  263 : Reward =  14.0\n",
      "Episode  264 : Reward =  64.0\n",
      "Episode  265 : Reward =  32.0\n",
      "Episode  266 : Reward =  11.0\n",
      "Episode  267 : Reward =  31.0\n",
      "Episode  268 : Reward =  21.0\n",
      "Episode  269 : Reward =  15.0\n",
      "Episode  270 : Reward =  12.0\n",
      "Episode  271 : Reward =  14.0\n",
      "Episode  272 : Reward =  20.0\n",
      "Episode  273 : Reward =  27.0\n",
      "Episode  274 : Reward =  13.0\n",
      "Episode  275 : Reward =  14.0\n",
      "Episode  276 : Reward =  17.0\n",
      "Episode  277 : Reward =  18.0\n",
      "Episode  278 : Reward =  16.0\n",
      "Episode  279 : Reward =  13.0\n",
      "Episode  280 : Reward =  10.0\n",
      "Episode  281 : Reward =  14.0\n",
      "Episode  282 : Reward =  39.0\n",
      "Episode  283 : Reward =  25.0\n",
      "Episode  284 : Reward =  20.0\n",
      "Episode  285 : Reward =  12.0\n",
      "Episode  286 : Reward =  18.0\n",
      "Episode  287 : Reward =  18.0\n",
      "Episode  288 : Reward =  40.0\n",
      "Episode  289 : Reward =  10.0\n",
      "Episode  290 : Reward =  18.0\n",
      "Episode  291 : Reward =  18.0\n",
      "Episode  292 : Reward =  14.0\n",
      "Episode  293 : Reward =  19.0\n",
      "Episode  294 : Reward =  12.0\n",
      "Episode  295 : Reward =  15.0\n",
      "Episode  296 : Reward =  36.0\n",
      "Episode  297 : Reward =  11.0\n",
      "Episode  298 : Reward =  10.0\n",
      "Episode  299 : Reward =  20.0\n",
      "Episode  300 : Reward =  23.0\n",
      "Episode  301 : Reward =  18.0\n",
      "Episode  302 : Reward =  17.0\n",
      "Episode  303 : Reward =  24.0\n",
      "Episode  304 : Reward =  59.0\n",
      "Episode  305 : Reward =  29.0\n",
      "Episode  306 : Reward =  24.0\n",
      "Episode  307 : Reward =  56.0\n",
      "Episode  308 : Reward =  10.0\n",
      "Episode  309 : Reward =  11.0\n",
      "Episode  310 : Reward =  28.0\n",
      "Episode  311 : Reward =  10.0\n",
      "Episode  312 : Reward =  29.0\n",
      "Episode  313 : Reward =  24.0\n",
      "Episode  314 : Reward =  12.0\n",
      "Episode  315 : Reward =  42.0\n",
      "Episode  316 : Reward =  19.0\n",
      "Episode  317 : Reward =  14.0\n",
      "Episode  318 : Reward =  15.0\n",
      "Episode  319 : Reward =  16.0\n",
      "Episode  320 : Reward =  51.0\n",
      "Episode  321 : Reward =  30.0\n",
      "Episode  322 : Reward =  55.0\n",
      "Episode  323 : Reward =  15.0\n",
      "Episode  324 : Reward =  14.0\n",
      "Episode  325 : Reward =  21.0\n",
      "Episode  326 : Reward =  16.0\n",
      "Episode  327 : Reward =  23.0\n",
      "Episode  328 : Reward =  15.0\n",
      "Episode  329 : Reward =  20.0\n",
      "Episode  330 : Reward =  31.0\n",
      "Episode  331 : Reward =  18.0\n",
      "Episode  332 : Reward =  12.0\n",
      "Episode  333 : Reward =  13.0\n",
      "Episode  334 : Reward =  20.0\n",
      "Episode  335 : Reward =  9.0\n",
      "Episode  336 : Reward =  16.0\n",
      "Episode  337 : Reward =  20.0\n",
      "Episode  338 : Reward =  14.0\n",
      "Episode  339 : Reward =  50.0\n",
      "Episode  340 : Reward =  13.0\n",
      "Episode  341 : Reward =  10.0\n",
      "Episode  342 : Reward =  23.0\n",
      "Episode  343 : Reward =  24.0\n",
      "Episode  344 : Reward =  22.0\n",
      "Episode  345 : Reward =  16.0\n",
      "Episode  346 : Reward =  19.0\n",
      "Episode  347 : Reward =  8.0\n",
      "Episode  348 : Reward =  14.0\n",
      "Episode  349 : Reward =  15.0\n",
      "Episode  350 : Reward =  13.0\n",
      "Episode  351 : Reward =  10.0\n",
      "Episode  352 : Reward =  28.0\n",
      "Episode  353 : Reward =  12.0\n",
      "Episode  354 : Reward =  18.0\n",
      "Episode  355 : Reward =  13.0\n",
      "Episode  356 : Reward =  22.0\n",
      "Episode  357 : Reward =  12.0\n",
      "Episode  358 : Reward =  13.0\n",
      "Episode  359 : Reward =  28.0\n",
      "Episode  360 : Reward =  20.0\n",
      "Episode  361 : Reward =  26.0\n",
      "Episode  362 : Reward =  14.0\n",
      "Episode  363 : Reward =  16.0\n",
      "Episode  364 : Reward =  24.0\n",
      "Episode  365 : Reward =  13.0\n",
      "Episode  366 : Reward =  25.0\n",
      "Episode  367 : Reward =  10.0\n",
      "Episode  368 : Reward =  29.0\n",
      "Episode  369 : Reward =  17.0\n",
      "Episode  370 : Reward =  38.0\n",
      "Episode  371 : Reward =  11.0\n",
      "Episode  372 : Reward =  21.0\n",
      "Episode  373 : Reward =  52.0\n",
      "Episode  374 : Reward =  27.0\n",
      "Episode  375 : Reward =  29.0\n",
      "Episode  376 : Reward =  37.0\n",
      "Episode  377 : Reward =  12.0\n",
      "Episode  378 : Reward =  12.0\n",
      "Episode  379 : Reward =  10.0\n",
      "Episode  380 : Reward =  44.0\n",
      "Episode  381 : Reward =  15.0\n",
      "Episode  382 : Reward =  54.0\n",
      "Episode  383 : Reward =  19.0\n",
      "Episode  384 : Reward =  8.0\n",
      "Episode  385 : Reward =  10.0\n",
      "Episode  386 : Reward =  12.0\n",
      "Episode  387 : Reward =  11.0\n",
      "Episode  388 : Reward =  41.0\n",
      "Episode  389 : Reward =  57.0\n",
      "Episode  390 : Reward =  9.0\n",
      "Episode  391 : Reward =  69.0\n",
      "Episode  392 : Reward =  36.0\n",
      "Episode  393 : Reward =  32.0\n",
      "Episode  394 : Reward =  71.0\n",
      "Episode  395 : Reward =  11.0\n",
      "Episode  396 : Reward =  22.0\n",
      "Episode  397 : Reward =  14.0\n",
      "Episode  398 : Reward =  12.0\n",
      "Episode  399 : Reward =  37.0\n",
      "Episode  400 : Reward =  14.0\n",
      "Episode  401 : Reward =  16.0\n",
      "Episode  402 : Reward =  13.0\n",
      "Episode  403 : Reward =  42.0\n",
      "Episode  404 : Reward =  15.0\n",
      "Episode  405 : Reward =  55.0\n",
      "Episode  406 : Reward =  36.0\n",
      "Episode  407 : Reward =  27.0\n",
      "Episode  408 : Reward =  27.0\n",
      "Episode  409 : Reward =  27.0\n",
      "Episode  410 : Reward =  23.0\n",
      "Episode  411 : Reward =  16.0\n",
      "Episode  412 : Reward =  29.0\n",
      "Episode  413 : Reward =  47.0\n",
      "Episode  414 : Reward =  117.0\n",
      "Saving better model at episode 414 with reward 117.0\n",
      "Episode  415 : Reward =  16.0\n",
      "Episode  416 : Reward =  35.0\n",
      "Episode  417 : Reward =  24.0\n",
      "Episode  418 : Reward =  39.0\n",
      "Episode  419 : Reward =  17.0\n",
      "Episode  420 : Reward =  51.0\n",
      "Episode  421 : Reward =  16.0\n",
      "Episode  422 : Reward =  16.0\n",
      "Episode  423 : Reward =  17.0\n",
      "Episode  424 : Reward =  16.0\n",
      "Episode  425 : Reward =  33.0\n",
      "Episode  426 : Reward =  15.0\n",
      "Episode  427 : Reward =  31.0\n",
      "Episode  428 : Reward =  27.0\n",
      "Episode  429 : Reward =  14.0\n",
      "Episode  430 : Reward =  42.0\n",
      "Episode  431 : Reward =  13.0\n",
      "Episode  432 : Reward =  22.0\n",
      "Episode  433 : Reward =  34.0\n",
      "Episode  434 : Reward =  24.0\n",
      "Episode  435 : Reward =  16.0\n",
      "Episode  436 : Reward =  16.0\n",
      "Episode  437 : Reward =  13.0\n",
      "Episode  438 : Reward =  26.0\n",
      "Episode  439 : Reward =  53.0\n",
      "Episode  440 : Reward =  40.0\n",
      "Episode  441 : Reward =  18.0\n",
      "Episode  442 : Reward =  38.0\n",
      "Episode  443 : Reward =  13.0\n",
      "Episode  444 : Reward =  63.0\n",
      "Episode  445 : Reward =  14.0\n",
      "Episode  446 : Reward =  18.0\n",
      "Episode  447 : Reward =  32.0\n",
      "Episode  448 : Reward =  15.0\n",
      "Episode  449 : Reward =  29.0\n",
      "Episode  450 : Reward =  11.0\n",
      "Episode  451 : Reward =  26.0\n",
      "Episode  452 : Reward =  16.0\n",
      "Episode  453 : Reward =  24.0\n",
      "Episode  454 : Reward =  34.0\n",
      "Episode  455 : Reward =  19.0\n",
      "Episode  456 : Reward =  9.0\n",
      "Episode  457 : Reward =  43.0\n",
      "Episode  458 : Reward =  32.0\n",
      "Episode  459 : Reward =  28.0\n",
      "Episode  460 : Reward =  33.0\n",
      "Episode  461 : Reward =  30.0\n",
      "Episode  462 : Reward =  43.0\n",
      "Episode  463 : Reward =  20.0\n",
      "Episode  464 : Reward =  31.0\n",
      "Episode  465 : Reward =  40.0\n",
      "Episode  466 : Reward =  19.0\n",
      "Episode  467 : Reward =  23.0\n",
      "Episode  468 : Reward =  19.0\n",
      "Episode  469 : Reward =  36.0\n",
      "Episode  470 : Reward =  23.0\n",
      "Episode  471 : Reward =  36.0\n",
      "Episode  472 : Reward =  32.0\n",
      "Episode  473 : Reward =  27.0\n",
      "Episode  474 : Reward =  43.0\n",
      "Episode  475 : Reward =  42.0\n",
      "Episode  476 : Reward =  43.0\n",
      "Episode  477 : Reward =  58.0\n",
      "Episode  478 : Reward =  38.0\n",
      "Episode  479 : Reward =  15.0\n",
      "Episode  480 : Reward =  27.0\n",
      "Episode  481 : Reward =  16.0\n",
      "Episode  482 : Reward =  18.0\n",
      "Episode  483 : Reward =  40.0\n",
      "Episode  484 : Reward =  49.0\n",
      "Episode  485 : Reward =  30.0\n",
      "Episode  486 : Reward =  17.0\n",
      "Episode  487 : Reward =  25.0\n",
      "Episode  488 : Reward =  77.0\n",
      "Episode  489 : Reward =  31.0\n",
      "Episode  490 : Reward =  28.0\n",
      "Episode  491 : Reward =  12.0\n",
      "Episode  492 : Reward =  51.0\n",
      "Episode  493 : Reward =  36.0\n",
      "Episode  494 : Reward =  19.0\n",
      "Episode  495 : Reward =  75.0\n",
      "Episode  496 : Reward =  16.0\n",
      "Episode  497 : Reward =  22.0\n",
      "Episode  498 : Reward =  17.0\n",
      "Episode  499 : Reward =  33.0\n",
      "Episode  500 : Reward =  12.0\n",
      "Episode  501 : Reward =  28.0\n",
      "Episode  502 : Reward =  21.0\n",
      "Episode  503 : Reward =  65.0\n",
      "Episode  504 : Reward =  11.0\n",
      "Episode  505 : Reward =  103.0\n",
      "Episode  506 : Reward =  14.0\n",
      "Episode  507 : Reward =  32.0\n",
      "Episode  508 : Reward =  50.0\n",
      "Episode  509 : Reward =  18.0\n",
      "Episode  510 : Reward =  41.0\n",
      "Episode  511 : Reward =  17.0\n",
      "Episode  512 : Reward =  34.0\n",
      "Episode  513 : Reward =  17.0\n",
      "Episode  514 : Reward =  24.0\n",
      "Episode  515 : Reward =  16.0\n",
      "Episode  516 : Reward =  15.0\n",
      "Episode  517 : Reward =  10.0\n",
      "Episode  518 : Reward =  24.0\n",
      "Episode  519 : Reward =  15.0\n",
      "Episode  520 : Reward =  28.0\n",
      "Episode  521 : Reward =  29.0\n",
      "Episode  522 : Reward =  15.0\n",
      "Episode  523 : Reward =  43.0\n",
      "Episode  524 : Reward =  30.0\n",
      "Episode  525 : Reward =  26.0\n",
      "Episode  526 : Reward =  91.0\n",
      "Episode  527 : Reward =  24.0\n",
      "Episode  528 : Reward =  23.0\n",
      "Episode  529 : Reward =  15.0\n",
      "Episode  530 : Reward =  20.0\n",
      "Episode  531 : Reward =  20.0\n",
      "Episode  532 : Reward =  25.0\n",
      "Episode  533 : Reward =  18.0\n",
      "Episode  534 : Reward =  38.0\n",
      "Episode  535 : Reward =  93.0\n",
      "Episode  536 : Reward =  10.0\n",
      "Episode  537 : Reward =  79.0\n",
      "Episode  538 : Reward =  20.0\n",
      "Episode  539 : Reward =  44.0\n",
      "Episode  540 : Reward =  96.0\n",
      "Episode  541 : Reward =  45.0\n",
      "Episode  542 : Reward =  31.0\n",
      "Episode  543 : Reward =  63.0\n",
      "Episode  544 : Reward =  73.0\n",
      "Episode  545 : Reward =  17.0\n",
      "Episode  546 : Reward =  65.0\n",
      "Episode  547 : Reward =  62.0\n",
      "Episode  548 : Reward =  72.0\n",
      "Episode  549 : Reward =  19.0\n",
      "Episode  550 : Reward =  50.0\n",
      "Episode  551 : Reward =  46.0\n",
      "Episode  552 : Reward =  21.0\n",
      "Episode  553 : Reward =  43.0\n",
      "Episode  554 : Reward =  27.0\n",
      "Episode  555 : Reward =  24.0\n",
      "Episode  556 : Reward =  40.0\n",
      "Episode  557 : Reward =  17.0\n",
      "Episode  558 : Reward =  23.0\n",
      "Episode  559 : Reward =  102.0\n",
      "Episode  560 : Reward =  67.0\n",
      "Episode  561 : Reward =  15.0\n",
      "Episode  562 : Reward =  24.0\n",
      "Episode  563 : Reward =  143.0\n",
      "Saving better model at episode 563 with reward 143.0\n",
      "Episode  564 : Reward =  54.0\n",
      "Episode  565 : Reward =  15.0\n",
      "Episode  566 : Reward =  27.0\n",
      "Episode  567 : Reward =  64.0\n",
      "Episode  568 : Reward =  61.0\n",
      "Episode  569 : Reward =  53.0\n",
      "Episode  570 : Reward =  37.0\n",
      "Episode  571 : Reward =  43.0\n",
      "Episode  572 : Reward =  12.0\n",
      "Episode  573 : Reward =  23.0\n",
      "Episode  574 : Reward =  52.0\n",
      "Episode  575 : Reward =  25.0\n",
      "Episode  576 : Reward =  20.0\n",
      "Episode  577 : Reward =  22.0\n",
      "Episode  578 : Reward =  31.0\n",
      "Episode  579 : Reward =  88.0\n",
      "Episode  580 : Reward =  12.0\n",
      "Episode  581 : Reward =  16.0\n",
      "Episode  582 : Reward =  46.0\n",
      "Episode  583 : Reward =  58.0\n",
      "Episode  584 : Reward =  17.0\n",
      "Episode  585 : Reward =  53.0\n",
      "Episode  586 : Reward =  11.0\n",
      "Episode  587 : Reward =  18.0\n",
      "Episode  588 : Reward =  58.0\n",
      "Episode  589 : Reward =  15.0\n",
      "Episode  590 : Reward =  19.0\n",
      "Episode  591 : Reward =  39.0\n",
      "Episode  592 : Reward =  35.0\n",
      "Episode  593 : Reward =  71.0\n",
      "Episode  594 : Reward =  25.0\n",
      "Episode  595 : Reward =  61.0\n",
      "Episode  596 : Reward =  21.0\n",
      "Episode  597 : Reward =  56.0\n",
      "Episode  598 : Reward =  38.0\n",
      "Episode  599 : Reward =  30.0\n",
      "Episode  600 : Reward =  18.0\n",
      "Episode  601 : Reward =  41.0\n",
      "Episode  602 : Reward =  33.0\n",
      "Episode  603 : Reward =  18.0\n",
      "Episode  604 : Reward =  15.0\n",
      "Episode  605 : Reward =  50.0\n",
      "Episode  606 : Reward =  43.0\n",
      "Episode  607 : Reward =  47.0\n",
      "Episode  608 : Reward =  31.0\n",
      "Episode  609 : Reward =  78.0\n",
      "Episode  610 : Reward =  32.0\n",
      "Episode  611 : Reward =  13.0\n",
      "Episode  612 : Reward =  70.0\n",
      "Episode  613 : Reward =  64.0\n",
      "Episode  614 : Reward =  14.0\n",
      "Episode  615 : Reward =  15.0\n",
      "Episode  616 : Reward =  88.0\n",
      "Episode  617 : Reward =  25.0\n",
      "Episode  618 : Reward =  37.0\n",
      "Episode  619 : Reward =  40.0\n",
      "Episode  620 : Reward =  18.0\n",
      "Episode  621 : Reward =  54.0\n",
      "Episode  622 : Reward =  47.0\n",
      "Episode  623 : Reward =  65.0\n",
      "Episode  624 : Reward =  37.0\n",
      "Episode  625 : Reward =  56.0\n",
      "Episode  626 : Reward =  71.0\n",
      "Episode  627 : Reward =  24.0\n",
      "Episode  628 : Reward =  110.0\n",
      "Episode  629 : Reward =  132.0\n",
      "Episode  630 : Reward =  197.0\n",
      "Saving better model at episode 630 with reward 197.0\n",
      "Episode  631 : Reward =  24.0\n",
      "Episode  632 : Reward =  136.0\n",
      "Episode  633 : Reward =  51.0\n",
      "Episode  634 : Reward =  46.0\n",
      "Episode  635 : Reward =  30.0\n",
      "Episode  636 : Reward =  31.0\n",
      "Episode  637 : Reward =  46.0\n",
      "Episode  638 : Reward =  81.0\n",
      "Episode  639 : Reward =  172.0\n",
      "Episode  640 : Reward =  117.0\n",
      "Episode  641 : Reward =  112.0\n",
      "Episode  642 : Reward =  144.0\n",
      "Episode  643 : Reward =  155.0\n",
      "Episode  644 : Reward =  36.0\n",
      "Episode  645 : Reward =  23.0\n",
      "Episode  646 : Reward =  72.0\n",
      "Episode  647 : Reward =  23.0\n",
      "Episode  648 : Reward =  87.0\n",
      "Episode  649 : Reward =  69.0\n",
      "Episode  650 : Reward =  84.0\n",
      "Episode  651 : Reward =  52.0\n",
      "Episode  652 : Reward =  39.0\n",
      "Episode  653 : Reward =  67.0\n",
      "Episode  654 : Reward =  120.0\n",
      "Episode  655 : Reward =  26.0\n",
      "Episode  656 : Reward =  51.0\n",
      "Episode  657 : Reward =  31.0\n",
      "Episode  658 : Reward =  90.0\n",
      "Episode  659 : Reward =  70.0\n",
      "Episode  660 : Reward =  131.0\n",
      "Episode  661 : Reward =  60.0\n",
      "Episode  662 : Reward =  73.0\n",
      "Episode  663 : Reward =  53.0\n",
      "Episode  664 : Reward =  16.0\n",
      "Episode  665 : Reward =  19.0\n",
      "Episode  666 : Reward =  42.0\n",
      "Episode  667 : Reward =  61.0\n",
      "Episode  668 : Reward =  28.0\n",
      "Episode  669 : Reward =  17.0\n",
      "Episode  670 : Reward =  20.0\n",
      "Episode  671 : Reward =  19.0\n",
      "Episode  672 : Reward =  24.0\n",
      "Episode  673 : Reward =  98.0\n",
      "Episode  674 : Reward =  114.0\n",
      "Episode  675 : Reward =  67.0\n",
      "Episode  676 : Reward =  81.0\n",
      "Episode  677 : Reward =  15.0\n",
      "Episode  678 : Reward =  14.0\n",
      "Episode  679 : Reward =  26.0\n",
      "Episode  680 : Reward =  32.0\n",
      "Episode  681 : Reward =  98.0\n",
      "Episode  682 : Reward =  66.0\n",
      "Episode  683 : Reward =  44.0\n",
      "Episode  684 : Reward =  100.0\n",
      "Episode  685 : Reward =  39.0\n",
      "Episode  686 : Reward =  122.0\n",
      "Episode  687 : Reward =  125.0\n",
      "Episode  688 : Reward =  85.0\n",
      "Episode  689 : Reward =  237.0\n",
      "Saving better model at episode 689 with reward 237.0\n",
      "Episode  690 : Reward =  62.0\n",
      "Episode  691 : Reward =  119.0\n",
      "Episode  692 : Reward =  125.0\n",
      "Episode  693 : Reward =  190.0\n",
      "Episode  694 : Reward =  185.0\n",
      "Episode  695 : Reward =  236.0\n",
      "Episode  696 : Reward =  66.0\n",
      "Episode  697 : Reward =  124.0\n",
      "Episode  698 : Reward =  117.0\n",
      "Episode  699 : Reward =  69.0\n",
      "Episode  700 : Reward =  54.0\n",
      "Episode  701 : Reward =  158.0\n",
      "Episode  702 : Reward =  165.0\n",
      "Episode  703 : Reward =  301.0\n",
      "Saving better model at episode 703 with reward 301.0\n",
      "Episode  704 : Reward =  87.0\n",
      "Episode  705 : Reward =  220.0\n",
      "Episode  706 : Reward =  185.0\n",
      "Episode  707 : Reward =  233.0\n",
      "Episode  708 : Reward =  136.0\n",
      "Episode  709 : Reward =  164.0\n",
      "Episode  710 : Reward =  23.0\n",
      "Episode  711 : Reward =  116.0\n",
      "Episode  712 : Reward =  152.0\n",
      "Episode  713 : Reward =  105.0\n",
      "Episode  714 : Reward =  291.0\n",
      "Episode  715 : Reward =  113.0\n",
      "Episode  716 : Reward =  251.0\n",
      "Episode  717 : Reward =  182.0\n",
      "Episode  718 : Reward =  66.0\n",
      "Episode  719 : Reward =  154.0\n",
      "Episode  720 : Reward =  137.0\n",
      "Episode  721 : Reward =  154.0\n",
      "Episode  722 : Reward =  123.0\n",
      "Episode  723 : Reward =  191.0\n",
      "Episode  724 : Reward =  172.0\n",
      "Episode  725 : Reward =  90.0\n",
      "Episode  726 : Reward =  138.0\n",
      "Episode  727 : Reward =  298.0\n",
      "Episode  728 : Reward =  139.0\n",
      "Episode  729 : Reward =  170.0\n",
      "Episode  730 : Reward =  209.0\n",
      "Episode  731 : Reward =  48.0\n",
      "Episode  732 : Reward =  137.0\n",
      "Episode  733 : Reward =  165.0\n",
      "Episode  734 : Reward =  237.0\n",
      "Episode  735 : Reward =  129.0\n",
      "Episode  736 : Reward =  230.0\n",
      "Episode  737 : Reward =  165.0\n",
      "Episode  738 : Reward =  467.0\n",
      "Saving better model at episode 738 with reward 467.0\n",
      "Episode  739 : Reward =  132.0\n",
      "Episode  740 : Reward =  122.0\n",
      "Episode  741 : Reward =  32.0\n",
      "Episode  742 : Reward =  271.0\n",
      "Episode  743 : Reward =  243.0\n",
      "Episode  744 : Reward =  207.0\n",
      "Episode  745 : Reward =  96.0\n",
      "Episode  746 : Reward =  165.0\n",
      "Episode  747 : Reward =  78.0\n",
      "Episode  748 : Reward =  224.0\n",
      "Episode  749 : Reward =  224.0\n",
      "Episode  750 : Reward =  286.0\n",
      "Episode  751 : Reward =  292.0\n",
      "Episode  752 : Reward =  144.0\n",
      "Episode  753 : Reward =  155.0\n",
      "Episode  754 : Reward =  376.0\n",
      "Episode  755 : Reward =  244.0\n",
      "Episode  756 : Reward =  146.0\n",
      "Episode  757 : Reward =  347.0\n",
      "Episode  758 : Reward =  176.0\n",
      "Episode  759 : Reward =  56.0\n",
      "Episode  760 : Reward =  141.0\n",
      "Episode  761 : Reward =  137.0\n",
      "Episode  762 : Reward =  109.0\n",
      "Episode  763 : Reward =  283.0\n",
      "Episode  764 : Reward =  230.0\n",
      "Episode  765 : Reward =  207.0\n",
      "Episode  766 : Reward =  219.0\n",
      "Episode  767 : Reward =  133.0\n",
      "Episode  768 : Reward =  265.0\n",
      "Episode  769 : Reward =  196.0\n",
      "Episode  770 : Reward =  320.0\n",
      "Episode  771 : Reward =  191.0\n",
      "Episode  772 : Reward =  471.0\n",
      "Saving better model at episode 772 with reward 471.0\n",
      "Episode  773 : Reward =  222.0\n",
      "Episode  774 : Reward =  77.0\n",
      "Episode  775 : Reward =  134.0\n",
      "Episode  776 : Reward =  197.0\n",
      "Episode  777 : Reward =  391.0\n",
      "Episode  778 : Reward =  430.0\n",
      "Episode  779 : Reward =  273.0\n",
      "Episode  780 : Reward =  335.0\n",
      "Episode  781 : Reward =  184.0\n",
      "Episode  782 : Reward =  179.0\n",
      "Episode  783 : Reward =  234.0\n",
      "Episode  784 : Reward =  198.0\n",
      "Episode  785 : Reward =  174.0\n",
      "Episode  786 : Reward =  316.0\n",
      "Episode  787 : Reward =  94.0\n",
      "Episode  788 : Reward =  156.0\n",
      "Episode  789 : Reward =  146.0\n",
      "Episode  790 : Reward =  302.0\n",
      "Episode  791 : Reward =  175.0\n",
      "Episode  792 : Reward =  88.0\n",
      "Episode  793 : Reward =  222.0\n",
      "Episode  794 : Reward =  250.0\n",
      "Episode  795 : Reward =  259.0\n",
      "Episode  796 : Reward =  349.0\n",
      "Episode  797 : Reward =  262.0\n",
      "Episode  798 : Reward =  472.0\n",
      "Saving better model at episode 798 with reward 472.0\n",
      "Episode  799 : Reward =  465.0\n",
      "Episode  800 : Reward =  229.0\n",
      "Episode  801 : Reward =  177.0\n",
      "Episode  802 : Reward =  275.0\n",
      "Episode  803 : Reward =  357.0\n",
      "Episode  804 : Reward =  181.0\n",
      "Episode  805 : Reward =  115.0\n",
      "Episode  806 : Reward =  266.0\n",
      "Episode  807 : Reward =  258.0\n",
      "Episode  808 : Reward =  301.0\n",
      "Episode  809 : Reward =  165.0\n",
      "Episode  810 : Reward =  321.0\n",
      "Episode  811 : Reward =  271.0\n",
      "Episode  812 : Reward =  289.0\n",
      "Episode  813 : Reward =  140.0\n",
      "Episode  814 : Reward =  105.0\n",
      "Episode  815 : Reward =  173.0\n",
      "Episode  816 : Reward =  143.0\n",
      "Episode  817 : Reward =  168.0\n",
      "Episode  818 : Reward =  320.0\n",
      "Episode  819 : Reward =  136.0\n",
      "Episode  820 : Reward =  306.0\n",
      "Episode  821 : Reward =  252.0\n",
      "Episode  822 : Reward =  195.0\n",
      "Episode  823 : Reward =  305.0\n",
      "Episode  824 : Reward =  270.0\n",
      "Episode  825 : Reward =  193.0\n",
      "Episode  826 : Reward =  487.0\n",
      "Saving better model at episode 826 with reward 487.0\n",
      "Episode  827 : Reward =  232.0\n",
      "Episode  828 : Reward =  271.0\n",
      "Episode  829 : Reward =  366.0\n",
      "Episode  830 : Reward =  470.0\n",
      "Episode  831 : Reward =  844.0\n",
      "Saving better model at episode 831 with reward 844.0\n",
      "Episode  832 : Reward =  562.0\n",
      "Episode  833 : Reward =  431.0\n",
      "Episode  834 : Reward =  597.0\n",
      "Episode  835 : Reward =  162.0\n",
      "Episode  836 : Reward =  743.0\n",
      "Episode  837 : Reward =  582.0\n",
      "Episode  838 : Reward =  841.0\n",
      "Episode  839 : Reward =  240.0\n",
      "Episode  840 : Reward =  383.0\n",
      "Episode  841 : Reward =  988.0\n",
      "Saving better model at episode 841 with reward 988.0\n",
      "Episode  842 : Reward =  574.0\n",
      "Episode  843 : Reward =  341.0\n",
      "Episode  844 : Reward =  946.0\n",
      "Episode  845 : Reward =  285.0\n",
      "Episode  846 : Reward =  321.0\n",
      "Episode  847 : Reward =  595.0\n",
      "Episode  848 : Reward =  1000.0\n",
      "Saving better model at episode 848 with reward 1000.0\n",
      "Episode  849 : Reward =  534.0\n",
      "Episode  850 : Reward =  529.0\n",
      "Episode  851 : Reward =  567.0\n",
      "Episode  852 : Reward =  684.0\n",
      "Episode  853 : Reward =  1000.0\n",
      "Saving better model at episode 853 with reward 1000.0\n",
      "Episode  854 : Reward =  256.0\n",
      "Episode  855 : Reward =  307.0\n",
      "Episode  856 : Reward =  550.0\n",
      "Episode  857 : Reward =  368.0\n",
      "Episode  858 : Reward =  738.0\n",
      "Episode  859 : Reward =  333.0\n",
      "Episode  860 : Reward =  332.0\n",
      "Episode  861 : Reward =  488.0\n",
      "Episode  862 : Reward =  288.0\n",
      "Episode  863 : Reward =  609.0\n",
      "Episode  864 : Reward =  385.0\n",
      "Episode  865 : Reward =  385.0\n",
      "Episode  866 : Reward =  274.0\n",
      "Episode  867 : Reward =  318.0\n",
      "Episode  868 : Reward =  537.0\n",
      "Episode  869 : Reward =  396.0\n",
      "Episode  870 : Reward =  409.0\n",
      "Episode  871 : Reward =  884.0\n",
      "Episode  872 : Reward =  1000.0\n",
      "Saving better model at episode 872 with reward 1000.0\n",
      "Episode  873 : Reward =  1000.0\n",
      "Saving better model at episode 873 with reward 1000.0\n",
      "Episode  874 : Reward =  1000.0\n",
      "Saving better model at episode 874 with reward 1000.0\n",
      "Episode  875 : Reward =  666.0\n",
      "Episode  876 : Reward =  1000.0\n",
      "Saving better model at episode 876 with reward 1000.0\n",
      "Episode  877 : Reward =  355.0\n",
      "Episode  878 : Reward =  356.0\n",
      "Episode  879 : Reward =  448.0\n",
      "Episode  880 : Reward =  461.0\n",
      "Episode  881 : Reward =  594.0\n",
      "Episode  882 : Reward =  415.0\n",
      "Episode  883 : Reward =  504.0\n",
      "Episode  884 : Reward =  281.0\n",
      "Episode  885 : Reward =  286.0\n",
      "Episode  886 : Reward =  1000.0\n",
      "Saving better model at episode 886 with reward 1000.0\n",
      "Episode  887 : Reward =  1000.0\n",
      "Saving better model at episode 887 with reward 1000.0\n",
      "Episode  888 : Reward =  1000.0\n",
      "Saving better model at episode 888 with reward 1000.0\n",
      "Episode  889 : Reward =  1000.0\n",
      "Saving better model at episode 889 with reward 1000.0\n",
      "Episode  890 : Reward =  580.0\n",
      "Episode  891 : Reward =  687.0\n",
      "Episode  892 : Reward =  209.0\n",
      "Episode  893 : Reward =  241.0\n",
      "Episode  894 : Reward =  511.0\n",
      "Episode  895 : Reward =  486.0\n",
      "Episode  896 : Reward =  605.0\n",
      "Episode  897 : Reward =  574.0\n",
      "Episode  898 : Reward =  147.0\n",
      "Episode  899 : Reward =  542.0\n",
      "Episode  900 : Reward =  1000.0\n",
      "Saving better model at episode 900 with reward 1000.0\n",
      "Episode  901 : Reward =  485.0\n",
      "Episode  902 : Reward =  1000.0\n",
      "Saving better model at episode 902 with reward 1000.0\n",
      "Episode  903 : Reward =  923.0\n",
      "Episode  904 : Reward =  1000.0\n",
      "Saving better model at episode 904 with reward 1000.0\n",
      "Episode  905 : Reward =  1000.0\n",
      "Saving better model at episode 905 with reward 1000.0\n",
      "Episode  906 : Reward =  312.0\n",
      "Episode  907 : Reward =  1000.0\n",
      "Saving better model at episode 907 with reward 1000.0\n",
      "Episode  908 : Reward =  106.0\n",
      "Episode  909 : Reward =  1000.0\n",
      "Saving better model at episode 909 with reward 1000.0\n",
      "Episode  910 : Reward =  587.0\n",
      "Episode  911 : Reward =  485.0\n",
      "Episode  912 : Reward =  691.0\n",
      "Episode  913 : Reward =  364.0\n",
      "Episode  914 : Reward =  1000.0\n",
      "Saving better model at episode 914 with reward 1000.0\n",
      "Episode  915 : Reward =  956.0\n",
      "Episode  916 : Reward =  648.0\n",
      "Episode  917 : Reward =  218.0\n",
      "Episode  918 : Reward =  443.0\n",
      "Episode  919 : Reward =  872.0\n",
      "Episode  920 : Reward =  722.0\n",
      "Episode  921 : Reward =  377.0\n",
      "Episode  922 : Reward =  458.0\n",
      "Episode  923 : Reward =  1000.0\n",
      "Saving better model at episode 923 with reward 1000.0\n",
      "Episode  924 : Reward =  1000.0\n",
      "Saving better model at episode 924 with reward 1000.0\n",
      "Episode  925 : Reward =  1000.0\n",
      "Saving better model at episode 925 with reward 1000.0\n",
      "Episode  926 : Reward =  1000.0\n",
      "Saving better model at episode 926 with reward 1000.0\n",
      "Episode  927 : Reward =  550.0\n",
      "Episode  928 : Reward =  445.0\n",
      "Episode  929 : Reward =  213.0\n",
      "Episode  930 : Reward =  245.0\n",
      "Episode  931 : Reward =  466.0\n",
      "Episode  932 : Reward =  425.0\n",
      "Episode  933 : Reward =  301.0\n",
      "Episode  934 : Reward =  319.0\n",
      "Episode  935 : Reward =  266.0\n",
      "Episode  936 : Reward =  226.0\n",
      "Episode  937 : Reward =  357.0\n",
      "Episode  938 : Reward =  229.0\n",
      "Episode  939 : Reward =  576.0\n",
      "Episode  940 : Reward =  482.0\n",
      "Episode  941 : Reward =  233.0\n",
      "Episode  942 : Reward =  333.0\n",
      "Episode  943 : Reward =  300.0\n",
      "Episode  944 : Reward =  897.0\n",
      "Episode  945 : Reward =  493.0\n",
      "Episode  946 : Reward =  693.0\n",
      "Episode  947 : Reward =  1000.0\n",
      "Saving better model at episode 947 with reward 1000.0\n",
      "Episode  948 : Reward =  1000.0\n",
      "Saving better model at episode 948 with reward 1000.0\n",
      "Episode  949 : Reward =  669.0\n",
      "Episode  950 : Reward =  482.0\n",
      "Episode  951 : Reward =  1000.0\n",
      "Saving better model at episode 951 with reward 1000.0\n",
      "Episode  952 : Reward =  569.0\n",
      "Episode  953 : Reward =  264.0\n",
      "Episode  954 : Reward =  234.0\n",
      "Episode  955 : Reward =  423.0\n",
      "Episode  956 : Reward =  276.0\n",
      "Episode  957 : Reward =  299.0\n",
      "Episode  958 : Reward =  547.0\n",
      "Episode  959 : Reward =  654.0\n",
      "Episode  960 : Reward =  364.0\n",
      "Episode  961 : Reward =  1000.0\n",
      "Saving better model at episode 961 with reward 1000.0\n",
      "Episode  962 : Reward =  1000.0\n",
      "Saving better model at episode 962 with reward 1000.0\n",
      "Episode  963 : Reward =  1000.0\n",
      "Saving better model at episode 963 with reward 1000.0\n",
      "Episode  964 : Reward =  1000.0\n",
      "Saving better model at episode 964 with reward 1000.0\n",
      "Episode  965 : Reward =  870.0\n",
      "Episode  966 : Reward =  518.0\n",
      "Episode  967 : Reward =  692.0\n",
      "Episode  968 : Reward =  898.0\n",
      "Episode  969 : Reward =  702.0\n",
      "Episode  970 : Reward =  1000.0\n",
      "Saving better model at episode 970 with reward 1000.0\n",
      "Episode  971 : Reward =  1000.0\n",
      "Saving better model at episode 971 with reward 1000.0\n",
      "Episode  972 : Reward =  704.0\n",
      "Episode  973 : Reward =  1000.0\n",
      "Saving better model at episode 973 with reward 1000.0\n",
      "Episode  974 : Reward =  1000.0\n",
      "Saving better model at episode 974 with reward 1000.0\n",
      "Episode  975 : Reward =  1000.0\n",
      "Saving better model at episode 975 with reward 1000.0\n",
      "Episode  976 : Reward =  413.0\n",
      "Episode  977 : Reward =  850.0\n",
      "Episode  978 : Reward =  490.0\n",
      "Episode  979 : Reward =  252.0\n",
      "Episode  980 : Reward =  285.0\n",
      "Episode  981 : Reward =  740.0\n",
      "Episode  982 : Reward =  1000.0\n",
      "Saving better model at episode 982 with reward 1000.0\n",
      "Episode  983 : Reward =  1000.0\n",
      "Saving better model at episode 983 with reward 1000.0\n",
      "Episode  984 : Reward =  1000.0\n",
      "Saving better model at episode 984 with reward 1000.0\n",
      "Episode  985 : Reward =  575.0\n",
      "Episode  986 : Reward =  794.0\n",
      "Episode  987 : Reward =  1000.0\n",
      "Saving better model at episode 987 with reward 1000.0\n",
      "Episode  988 : Reward =  1000.0\n",
      "Saving better model at episode 988 with reward 1000.0\n",
      "Episode  989 : Reward =  1000.0\n",
      "Saving better model at episode 989 with reward 1000.0\n",
      "Episode  990 : Reward =  1000.0\n",
      "Saving better model at episode 990 with reward 1000.0\n",
      "Episode  991 : Reward =  1000.0\n",
      "Saving better model at episode 991 with reward 1000.0\n",
      "Episode  992 : Reward =  826.0\n",
      "Episode  993 : Reward =  748.0\n",
      "Episode  994 : Reward =  1000.0\n",
      "Saving better model at episode 994 with reward 1000.0\n",
      "Episode  995 : Reward =  313.0\n",
      "Episode  996 : Reward =  411.0\n",
      "Episode  997 : Reward =  537.0\n",
      "Episode  998 : Reward =  651.0\n",
      "Episode  999 : Reward =  331.0\n",
      "Episode  1000 : Reward =  473.0\n",
      "Episode  1001 : Reward =  536.0\n",
      "Episode  1002 : Reward =  226.0\n",
      "Episode  1003 : Reward =  694.0\n",
      "Episode  1004 : Reward =  595.0\n",
      "Episode  1005 : Reward =  887.0\n",
      "Episode  1006 : Reward =  911.0\n",
      "Episode  1007 : Reward =  720.0\n",
      "Episode  1008 : Reward =  1000.0\n",
      "Saving better model at episode 1008 with reward 1000.0\n",
      "Episode  1009 : Reward =  732.0\n",
      "Episode  1010 : Reward =  1000.0\n",
      "Saving better model at episode 1010 with reward 1000.0\n",
      "Episode  1011 : Reward =  997.0\n",
      "Episode  1012 : Reward =  1000.0\n",
      "Saving better model at episode 1012 with reward 1000.0\n",
      "Episode  1013 : Reward =  1000.0\n",
      "Saving better model at episode 1013 with reward 1000.0\n",
      "Episode  1014 : Reward =  838.0\n",
      "Episode  1015 : Reward =  995.0\n",
      "Episode  1016 : Reward =  1000.0\n",
      "Saving better model at episode 1016 with reward 1000.0\n",
      "Episode  1017 : Reward =  712.0\n",
      "Episode  1018 : Reward =  1000.0\n",
      "Saving better model at episode 1018 with reward 1000.0\n",
      "Episode  1019 : Reward =  1000.0\n",
      "Saving better model at episode 1019 with reward 1000.0\n",
      "Episode  1020 : Reward =  1000.0\n",
      "Saving better model at episode 1020 with reward 1000.0\n",
      "Episode  1021 : Reward =  1000.0\n",
      "Saving better model at episode 1021 with reward 1000.0\n",
      "Episode  1022 : Reward =  1000.0\n",
      "Saving better model at episode 1022 with reward 1000.0\n",
      "Episode  1023 : Reward =  991.0\n",
      "Episode  1024 : Reward =  1000.0\n",
      "Saving better model at episode 1024 with reward 1000.0\n",
      "Episode  1025 : Reward =  1000.0\n",
      "Saving better model at episode 1025 with reward 1000.0\n",
      "Episode  1026 : Reward =  1000.0\n",
      "Saving better model at episode 1026 with reward 1000.0\n",
      "Episode  1027 : Reward =  1000.0\n",
      "Saving better model at episode 1027 with reward 1000.0\n",
      "Episode  1028 : Reward =  1000.0\n",
      "Saving better model at episode 1028 with reward 1000.0\n",
      "Episode  1029 : Reward =  1000.0\n",
      "Saving better model at episode 1029 with reward 1000.0\n",
      "Episode  1030 : Reward =  1000.0\n",
      "Saving better model at episode 1030 with reward 1000.0\n",
      "Episode  1031 : Reward =  1000.0\n",
      "Saving better model at episode 1031 with reward 1000.0\n",
      "Episode  1032 : Reward =  1000.0\n",
      "Saving better model at episode 1032 with reward 1000.0\n",
      "Episode  1033 : Reward =  1000.0\n",
      "Saving better model at episode 1033 with reward 1000.0\n",
      "Episode  1034 : Reward =  1000.0\n",
      "Saving better model at episode 1034 with reward 1000.0\n",
      "Episode  1035 : Reward =  1000.0\n",
      "Saving better model at episode 1035 with reward 1000.0\n",
      "Episode  1036 : Reward =  1000.0\n",
      "Saving better model at episode 1036 with reward 1000.0\n",
      "Episode  1037 : Reward =  1000.0\n",
      "Saving better model at episode 1037 with reward 1000.0\n",
      "Episode  1038 : Reward =  1000.0\n",
      "Saving better model at episode 1038 with reward 1000.0\n",
      "Episode  1039 : Reward =  1000.0\n",
      "Saving better model at episode 1039 with reward 1000.0\n",
      "Episode  1040 : Reward =  1000.0\n",
      "Saving better model at episode 1040 with reward 1000.0\n",
      "Episode  1041 : Reward =  1000.0\n",
      "Saving better model at episode 1041 with reward 1000.0\n",
      "Episode  1042 : Reward =  1000.0\n",
      "Saving better model at episode 1042 with reward 1000.0\n",
      "Episode  1043 : Reward =  1000.0\n",
      "Saving better model at episode 1043 with reward 1000.0\n",
      "Episode  1044 : Reward =  1000.0\n",
      "Saving better model at episode 1044 with reward 1000.0\n",
      "Episode  1045 : Reward =  1000.0\n",
      "Saving better model at episode 1045 with reward 1000.0\n",
      "Episode  1046 : Reward =  1000.0\n",
      "Saving better model at episode 1046 with reward 1000.0\n",
      "Episode  1047 : Reward =  863.0\n",
      "Episode  1048 : Reward =  1000.0\n",
      "Saving better model at episode 1048 with reward 1000.0\n",
      "Episode  1049 : Reward =  1000.0\n",
      "Saving better model at episode 1049 with reward 1000.0\n",
      "Episode  1050 : Reward =  1000.0\n",
      "Saving better model at episode 1050 with reward 1000.0\n",
      "Episode  1051 : Reward =  898.0\n",
      "Episode  1052 : Reward =  1000.0\n",
      "Saving better model at episode 1052 with reward 1000.0\n",
      "Episode  1053 : Reward =  1000.0\n",
      "Saving better model at episode 1053 with reward 1000.0\n",
      "Episode  1054 : Reward =  1000.0\n",
      "Saving better model at episode 1054 with reward 1000.0\n",
      "Episode  1055 : Reward =  1000.0\n",
      "Saving better model at episode 1055 with reward 1000.0\n",
      "Episode  1056 : Reward =  1000.0\n",
      "Saving better model at episode 1056 with reward 1000.0\n",
      "Episode  1057 : Reward =  1000.0\n",
      "Saving better model at episode 1057 with reward 1000.0\n",
      "Episode  1058 : Reward =  1000.0\n",
      "Saving better model at episode 1058 with reward 1000.0\n",
      "Episode  1059 : Reward =  1000.0\n",
      "Saving better model at episode 1059 with reward 1000.0\n",
      "Episode  1060 : Reward =  1000.0\n",
      "Saving better model at episode 1060 with reward 1000.0\n",
      "Episode  1061 : Reward =  1000.0\n",
      "Saving better model at episode 1061 with reward 1000.0\n",
      "Episode  1062 : Reward =  1000.0\n",
      "Saving better model at episode 1062 with reward 1000.0\n",
      "Episode  1063 : Reward =  1000.0\n",
      "Saving better model at episode 1063 with reward 1000.0\n",
      "Episode  1064 : Reward =  1000.0\n",
      "Saving better model at episode 1064 with reward 1000.0\n",
      "Episode  1065 : Reward =  1000.0\n",
      "Saving better model at episode 1065 with reward 1000.0\n",
      "Episode  1066 : Reward =  929.0\n",
      "Episode  1067 : Reward =  288.0\n",
      "Episode  1068 : Reward =  747.0\n",
      "Episode  1069 : Reward =  622.0\n",
      "Episode  1070 : Reward =  261.0\n",
      "Episode  1071 : Reward =  1000.0\n",
      "Saving better model at episode 1071 with reward 1000.0\n",
      "Episode  1072 : Reward =  631.0\n",
      "Episode  1073 : Reward =  1000.0\n",
      "Saving better model at episode 1073 with reward 1000.0\n",
      "Episode  1074 : Reward =  656.0\n",
      "Episode  1075 : Reward =  1000.0\n",
      "Saving better model at episode 1075 with reward 1000.0\n",
      "Episode  1076 : Reward =  1000.0\n",
      "Saving better model at episode 1076 with reward 1000.0\n",
      "Episode  1077 : Reward =  1000.0\n",
      "Saving better model at episode 1077 with reward 1000.0\n",
      "Episode  1078 : Reward =  930.0\n",
      "Episode  1079 : Reward =  1000.0\n",
      "Saving better model at episode 1079 with reward 1000.0\n",
      "Episode  1080 : Reward =  1000.0\n",
      "Saving better model at episode 1080 with reward 1000.0\n",
      "Episode  1081 : Reward =  1000.0\n",
      "Saving better model at episode 1081 with reward 1000.0\n",
      "Episode  1082 : Reward =  770.0\n",
      "Episode  1083 : Reward =  1000.0\n",
      "Saving better model at episode 1083 with reward 1000.0\n",
      "Episode  1084 : Reward =  1000.0\n",
      "Saving better model at episode 1084 with reward 1000.0\n",
      "Episode  1085 : Reward =  1000.0\n",
      "Saving better model at episode 1085 with reward 1000.0\n",
      "Episode  1086 : Reward =  1000.0\n",
      "Saving better model at episode 1086 with reward 1000.0\n",
      "Episode  1087 : Reward =  1000.0\n",
      "Saving better model at episode 1087 with reward 1000.0\n",
      "Episode  1088 : Reward =  1000.0\n",
      "Saving better model at episode 1088 with reward 1000.0\n",
      "Episode  1089 : Reward =  1000.0\n",
      "Saving better model at episode 1089 with reward 1000.0\n",
      "Episode  1090 : Reward =  1000.0\n",
      "Saving better model at episode 1090 with reward 1000.0\n",
      "Episode  1091 : Reward =  55.0\n",
      "Episode  1092 : Reward =  1000.0\n",
      "Saving better model at episode 1092 with reward 1000.0\n",
      "Episode  1093 : Reward =  1000.0\n",
      "Saving better model at episode 1093 with reward 1000.0\n",
      "Episode  1094 : Reward =  1000.0\n",
      "Saving better model at episode 1094 with reward 1000.0\n",
      "Episode  1095 : Reward =  1000.0\n",
      "Saving better model at episode 1095 with reward 1000.0\n",
      "Episode  1096 : Reward =  1000.0\n",
      "Saving better model at episode 1096 with reward 1000.0\n",
      "Episode  1097 : Reward =  1000.0\n",
      "Saving better model at episode 1097 with reward 1000.0\n",
      "Episode  1098 : Reward =  1000.0\n",
      "Saving better model at episode 1098 with reward 1000.0\n",
      "Episode  1099 : Reward =  1000.0\n",
      "Saving better model at episode 1099 with reward 1000.0\n",
      "Episode  1100 : Reward =  1000.0\n",
      "Saving better model at episode 1100 with reward 1000.0\n",
      "Episode  1101 : Reward =  1000.0\n",
      "Saving better model at episode 1101 with reward 1000.0\n",
      "Episode  1102 : Reward =  568.0\n",
      "Episode  1103 : Reward =  982.0\n",
      "Episode  1104 : Reward =  386.0\n",
      "Episode  1105 : Reward =  1000.0\n",
      "Saving better model at episode 1105 with reward 1000.0\n",
      "Episode  1106 : Reward =  772.0\n",
      "Episode  1107 : Reward =  915.0\n",
      "Episode  1108 : Reward =  628.0\n",
      "Episode  1109 : Reward =  506.0\n",
      "Episode  1110 : Reward =  1000.0\n",
      "Saving better model at episode 1110 with reward 1000.0\n",
      "Episode  1111 : Reward =  566.0\n",
      "Episode  1112 : Reward =  1000.0\n",
      "Saving better model at episode 1112 with reward 1000.0\n",
      "Episode  1113 : Reward =  630.0\n",
      "Episode  1114 : Reward =  1000.0\n",
      "Saving better model at episode 1114 with reward 1000.0\n",
      "Episode  1115 : Reward =  426.0\n",
      "Episode  1116 : Reward =  12.0\n",
      "Episode  1117 : Reward =  988.0\n",
      "Episode  1118 : Reward =  1000.0\n",
      "Saving better model at episode 1118 with reward 1000.0\n",
      "Episode  1119 : Reward =  1000.0\n",
      "Saving better model at episode 1119 with reward 1000.0\n",
      "Episode  1120 : Reward =  1000.0\n",
      "Saving better model at episode 1120 with reward 1000.0\n",
      "Episode  1121 : Reward =  1000.0\n",
      "Saving better model at episode 1121 with reward 1000.0\n",
      "Episode  1122 : Reward =  1000.0\n",
      "Saving better model at episode 1122 with reward 1000.0\n",
      "Episode  1123 : Reward =  1000.0\n",
      "Saving better model at episode 1123 with reward 1000.0\n",
      "Episode  1124 : Reward =  427.0\n",
      "Episode  1125 : Reward =  687.0\n",
      "Episode  1126 : Reward =  1000.0\n",
      "Saving better model at episode 1126 with reward 1000.0\n",
      "Episode  1127 : Reward =  1000.0\n",
      "Saving better model at episode 1127 with reward 1000.0\n",
      "Episode  1128 : Reward =  253.0\n",
      "Episode  1129 : Reward =  349.0\n",
      "Episode  1130 : Reward =  633.0\n",
      "Episode  1131 : Reward =  412.0\n",
      "Episode  1132 : Reward =  1000.0\n",
      "Saving better model at episode 1132 with reward 1000.0\n",
      "Episode  1133 : Reward =  1000.0\n",
      "Saving better model at episode 1133 with reward 1000.0\n",
      "Episode  1134 : Reward =  1000.0\n",
      "Saving better model at episode 1134 with reward 1000.0\n",
      "Episode  1135 : Reward =  1000.0\n",
      "Saving better model at episode 1135 with reward 1000.0\n",
      "Episode  1136 : Reward =  1000.0\n",
      "Saving better model at episode 1136 with reward 1000.0\n",
      "Episode  1137 : Reward =  996.0\n",
      "Episode  1138 : Reward =  1000.0\n",
      "Saving better model at episode 1138 with reward 1000.0\n",
      "Episode  1139 : Reward =  1000.0\n",
      "Saving better model at episode 1139 with reward 1000.0\n",
      "Episode  1140 : Reward =  180.0\n",
      "Episode  1141 : Reward =  254.0\n",
      "Episode  1142 : Reward =  1000.0\n",
      "Saving better model at episode 1142 with reward 1000.0\n",
      "Episode  1143 : Reward =  1000.0\n",
      "Saving better model at episode 1143 with reward 1000.0\n",
      "Episode  1144 : Reward =  1000.0\n",
      "Saving better model at episode 1144 with reward 1000.0\n",
      "Episode  1145 : Reward =  1000.0\n",
      "Saving better model at episode 1145 with reward 1000.0\n",
      "Episode  1146 : Reward =  1000.0\n",
      "Saving better model at episode 1146 with reward 1000.0\n",
      "Episode  1147 : Reward =  1000.0\n",
      "Saving better model at episode 1147 with reward 1000.0\n",
      "Episode  1148 : Reward =  1000.0\n",
      "Saving better model at episode 1148 with reward 1000.0\n",
      "Episode  1149 : Reward =  1000.0\n",
      "Saving better model at episode 1149 with reward 1000.0\n",
      "Episode  1150 : Reward =  1000.0\n",
      "Saving better model at episode 1150 with reward 1000.0\n",
      "Episode  1151 : Reward =  1000.0\n",
      "Saving better model at episode 1151 with reward 1000.0\n",
      "Episode  1152 : Reward =  556.0\n",
      "Episode  1153 : Reward =  944.0\n",
      "Episode  1154 : Reward =  1000.0\n",
      "Saving better model at episode 1154 with reward 1000.0\n",
      "Episode  1155 : Reward =  1000.0\n",
      "Saving better model at episode 1155 with reward 1000.0\n",
      "Episode  1156 : Reward =  1000.0\n",
      "Saving better model at episode 1156 with reward 1000.0\n",
      "Episode  1157 : Reward =  373.0\n",
      "Episode  1158 : Reward =  1000.0\n",
      "Saving better model at episode 1158 with reward 1000.0\n",
      "Episode  1159 : Reward =  969.0\n",
      "Episode  1160 : Reward =  1000.0\n",
      "Saving better model at episode 1160 with reward 1000.0\n",
      "Episode  1161 : Reward =  1000.0\n",
      "Saving better model at episode 1161 with reward 1000.0\n",
      "Episode  1162 : Reward =  1000.0\n",
      "Saving better model at episode 1162 with reward 1000.0\n",
      "Episode  1163 : Reward =  1000.0\n",
      "Saving better model at episode 1163 with reward 1000.0\n",
      "Episode  1164 : Reward =  1000.0\n",
      "Saving better model at episode 1164 with reward 1000.0\n"
     ]
    }
   ],
   "source": [
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "has_continuous_action_space = False #không gian action rời rạc\n",
    "best_reward = -float('inf')  # ban đầu là âm vô cực\n",
    "max_ep_len = 1000                    # max timesteps in one episode\n",
    "max_training_timesteps = int(3*1e5)   # break training loop if timeteps > max_training_timesteps -- dừng train khi số bước vượt quá\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "\n",
    "\n",
    "action_std = None\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps -- cập nhật policy\n",
    "K_epochs = 40               # update policy for K epochs -- số lần lặp tối ưu trên 1 batch dữ liệu\n",
    "eps_clip = 0.2              # clip parameter for PPO \n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "print(\"training environment name : \" + env_name)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "# print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "# print_running_reward = 0\n",
    "# print_running_episodes = 0\n",
    "\n",
    "# log_running_reward = 0\n",
    "# log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps: #vòng lặp dừng khi vượt quá max_time_step\n",
    "\n",
    "    state = env.reset() #trả về trạng thái ban đầu \n",
    "\n",
    "    # Check if state is tuple\n",
    "    if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "    current_ep_reward = 0 #tổng phần thưởng trong episode hiện tại\n",
    "\n",
    "    for t in range(1, max_ep_len+1): #chạy tối đa 500 bước\n",
    "\n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state) #chọn action cho state dự vào policy hiện tại\n",
    "        state, reward, done, _ , _= env.step(action) #thực hiện hành động và nhận phản hồi\n",
    "\n",
    "        # saving reward and is_terminals vào buffer\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        )\n",
    "\n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update() #cập nhật lại policy nếu đủ số bước\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        # Nếu không gian hành động là liên tục, giảm độ lệch chuẩn để chuyển từ khám phá sang khai thác\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "\n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    print('Episode ', i_episode, ': Reward = ', current_ep_reward)\n",
    "\n",
    "    # print_running_reward += current_ep_reward\n",
    "    # print_running_episodes += 1\n",
    "\n",
    "    # log_running_reward += current_ep_reward\n",
    "    # log_running_episodes += 1\n",
    "    # Save model if reward improves\n",
    "    \n",
    "    if current_ep_reward >= best_reward:\n",
    "        print(f\"Saving better model at episode {i_episode} with reward {current_ep_reward}\")\n",
    "        best_reward = current_ep_reward\n",
    "        torch.save(ppo_agent.policy.state_dict(), 'ppo_best_model.pth')\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7898987",
   "metadata": {},
   "source": [
    "EVALUATION MODEL PPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f02e634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Action = 0, Reward = 1.00\n",
      "Step 1: Action = 1, Reward = 1.00\n",
      "Step 2: Action = 0, Reward = 1.00\n",
      "Step 3: Action = 1, Reward = 1.00\n",
      "Step 4: Action = 0, Reward = 1.00\n",
      "Step 5: Action = 1, Reward = 1.00\n",
      "Step 6: Action = 0, Reward = 1.00\n",
      "Step 7: Action = 1, Reward = 1.00\n",
      "Step 8: Action = 1, Reward = 1.00\n",
      "Step 9: Action = 0, Reward = 1.00\n",
      "Step 10: Action = 0, Reward = 1.00\n",
      "Step 11: Action = 1, Reward = 1.00\n",
      "Step 12: Action = 1, Reward = 1.00\n",
      "Step 13: Action = 0, Reward = 1.00\n",
      "Step 14: Action = 0, Reward = 1.00\n",
      "Step 15: Action = 1, Reward = 1.00\n",
      "Step 16: Action = 1, Reward = 1.00\n",
      "Step 17: Action = 0, Reward = 1.00\n",
      "Step 18: Action = 0, Reward = 1.00\n",
      "Step 19: Action = 1, Reward = 1.00\n",
      "Step 20: Action = 0, Reward = 1.00\n",
      "Step 21: Action = 1, Reward = 1.00\n",
      "Step 22: Action = 1, Reward = 1.00\n",
      "Step 23: Action = 0, Reward = 1.00\n",
      "Step 24: Action = 1, Reward = 1.00\n",
      "Step 25: Action = 0, Reward = 1.00\n",
      "Step 26: Action = 0, Reward = 1.00\n",
      "Step 27: Action = 1, Reward = 1.00\n",
      "Step 28: Action = 0, Reward = 1.00\n",
      "Step 29: Action = 1, Reward = 1.00\n",
      "Step 30: Action = 1, Reward = 1.00\n",
      "Step 31: Action = 0, Reward = 1.00\n",
      "Step 32: Action = 1, Reward = 1.00\n",
      "Step 33: Action = 0, Reward = 1.00\n",
      "Step 34: Action = 0, Reward = 1.00\n",
      "Step 35: Action = 1, Reward = 1.00\n",
      "Step 36: Action = 0, Reward = 1.00\n",
      "Step 37: Action = 1, Reward = 1.00\n",
      "Step 38: Action = 1, Reward = 1.00\n",
      "Step 39: Action = 0, Reward = 1.00\n",
      "Step 40: Action = 0, Reward = 1.00\n",
      "Step 41: Action = 1, Reward = 1.00\n",
      "Step 42: Action = 1, Reward = 1.00\n",
      "Step 43: Action = 0, Reward = 1.00\n",
      "Step 44: Action = 1, Reward = 1.00\n",
      "Step 45: Action = 0, Reward = 1.00\n",
      "Step 46: Action = 0, Reward = 1.00\n",
      "Step 47: Action = 1, Reward = 1.00\n",
      "Step 48: Action = 0, Reward = 1.00\n",
      "Step 49: Action = 1, Reward = 1.00\n",
      "Step 50: Action = 1, Reward = 1.00\n",
      "Step 51: Action = 0, Reward = 1.00\n",
      "Step 52: Action = 0, Reward = 1.00\n",
      "Step 53: Action = 1, Reward = 1.00\n",
      "Step 54: Action = 1, Reward = 1.00\n",
      "Step 55: Action = 0, Reward = 1.00\n",
      "Step 56: Action = 1, Reward = 1.00\n",
      "Step 57: Action = 0, Reward = 1.00\n",
      "Step 58: Action = 0, Reward = 1.00\n",
      "Step 59: Action = 1, Reward = 1.00\n",
      "Step 60: Action = 0, Reward = 1.00\n",
      "Step 61: Action = 1, Reward = 1.00\n",
      "Step 62: Action = 1, Reward = 1.00\n",
      "Step 63: Action = 0, Reward = 1.00\n",
      "Step 64: Action = 1, Reward = 1.00\n",
      "Step 65: Action = 0, Reward = 1.00\n",
      "Step 66: Action = 0, Reward = 1.00\n",
      "Step 67: Action = 1, Reward = 1.00\n",
      "Step 68: Action = 0, Reward = 1.00\n",
      "Step 69: Action = 1, Reward = 1.00\n",
      "Step 70: Action = 1, Reward = 1.00\n",
      "Step 71: Action = 0, Reward = 1.00\n",
      "Step 72: Action = 1, Reward = 1.00\n",
      "Step 73: Action = 0, Reward = 1.00\n",
      "Step 74: Action = 0, Reward = 1.00\n",
      "Step 75: Action = 1, Reward = 1.00\n",
      "Step 76: Action = 0, Reward = 1.00\n",
      "Step 77: Action = 1, Reward = 1.00\n",
      "Step 78: Action = 1, Reward = 1.00\n",
      "Step 79: Action = 0, Reward = 1.00\n",
      "Step 80: Action = 0, Reward = 1.00\n",
      "Step 81: Action = 1, Reward = 1.00\n",
      "Step 82: Action = 1, Reward = 1.00\n",
      "Step 83: Action = 0, Reward = 1.00\n",
      "Step 84: Action = 1, Reward = 1.00\n",
      "Step 85: Action = 0, Reward = 1.00\n",
      "Step 86: Action = 0, Reward = 1.00\n",
      "Step 87: Action = 1, Reward = 1.00\n",
      "Step 88: Action = 0, Reward = 1.00\n",
      "Step 89: Action = 1, Reward = 1.00\n",
      "Step 90: Action = 1, Reward = 1.00\n",
      "Step 91: Action = 0, Reward = 1.00\n",
      "Step 92: Action = 0, Reward = 1.00\n",
      "Step 93: Action = 1, Reward = 1.00\n",
      "Step 94: Action = 1, Reward = 1.00\n",
      "Step 95: Action = 0, Reward = 1.00\n",
      "Step 96: Action = 1, Reward = 1.00\n",
      "Step 97: Action = 0, Reward = 1.00\n",
      "Step 98: Action = 0, Reward = 1.00\n",
      "Step 99: Action = 1, Reward = 1.00\n",
      "Step 100: Action = 0, Reward = 1.00\n",
      "Step 101: Action = 1, Reward = 1.00\n",
      "Step 102: Action = 1, Reward = 1.00\n",
      "Step 103: Action = 0, Reward = 1.00\n",
      "Step 104: Action = 1, Reward = 1.00\n",
      "Step 105: Action = 0, Reward = 1.00\n",
      "Step 106: Action = 0, Reward = 1.00\n",
      "Step 107: Action = 1, Reward = 1.00\n",
      "Step 108: Action = 0, Reward = 1.00\n",
      "Step 109: Action = 1, Reward = 1.00\n",
      "Step 110: Action = 1, Reward = 1.00\n",
      "Step 111: Action = 0, Reward = 1.00\n",
      "Step 112: Action = 0, Reward = 1.00\n",
      "Step 113: Action = 1, Reward = 1.00\n",
      "Step 114: Action = 1, Reward = 1.00\n",
      "Step 115: Action = 0, Reward = 1.00\n",
      "Step 116: Action = 1, Reward = 1.00\n",
      "Step 117: Action = 0, Reward = 1.00\n",
      "Step 118: Action = 0, Reward = 1.00\n",
      "Step 119: Action = 1, Reward = 1.00\n",
      "Step 120: Action = 0, Reward = 1.00\n",
      "Step 121: Action = 1, Reward = 1.00\n",
      "Step 122: Action = 1, Reward = 1.00\n",
      "Step 123: Action = 0, Reward = 1.00\n",
      "Step 124: Action = 0, Reward = 1.00\n",
      "Step 125: Action = 1, Reward = 1.00\n",
      "Step 126: Action = 1, Reward = 1.00\n",
      "Step 127: Action = 0, Reward = 1.00\n",
      "Step 128: Action = 1, Reward = 1.00\n",
      "Step 129: Action = 0, Reward = 1.00\n",
      "Step 130: Action = 0, Reward = 1.00\n",
      "Step 131: Action = 1, Reward = 1.00\n",
      "Step 132: Action = 0, Reward = 1.00\n",
      "Step 133: Action = 1, Reward = 1.00\n",
      "Step 134: Action = 1, Reward = 1.00\n",
      "Step 135: Action = 0, Reward = 1.00\n",
      "Step 136: Action = 0, Reward = 1.00\n",
      "Step 137: Action = 1, Reward = 1.00\n",
      "Step 138: Action = 1, Reward = 1.00\n",
      "Step 139: Action = 0, Reward = 1.00\n",
      "Step 140: Action = 1, Reward = 1.00\n",
      "Step 141: Action = 0, Reward = 1.00\n",
      "Step 142: Action = 0, Reward = 1.00\n",
      "Step 143: Action = 1, Reward = 1.00\n",
      "Step 144: Action = 0, Reward = 1.00\n",
      "Step 145: Action = 1, Reward = 1.00\n",
      "Step 146: Action = 1, Reward = 1.00\n",
      "Step 147: Action = 0, Reward = 1.00\n",
      "Step 148: Action = 0, Reward = 1.00\n",
      "Step 149: Action = 1, Reward = 1.00\n",
      "Step 150: Action = 1, Reward = 1.00\n",
      "Step 151: Action = 0, Reward = 1.00\n",
      "Step 152: Action = 1, Reward = 1.00\n",
      "Step 153: Action = 0, Reward = 1.00\n",
      "Step 154: Action = 0, Reward = 1.00\n",
      "Step 155: Action = 1, Reward = 1.00\n",
      "Step 156: Action = 0, Reward = 1.00\n",
      "Step 157: Action = 1, Reward = 1.00\n",
      "Step 158: Action = 1, Reward = 1.00\n",
      "Step 159: Action = 0, Reward = 1.00\n",
      "Step 160: Action = 0, Reward = 1.00\n",
      "Step 161: Action = 1, Reward = 1.00\n",
      "Step 162: Action = 1, Reward = 1.00\n",
      "Step 163: Action = 0, Reward = 1.00\n",
      "Step 164: Action = 1, Reward = 1.00\n",
      "Step 165: Action = 0, Reward = 1.00\n",
      "Step 166: Action = 0, Reward = 1.00\n",
      "Step 167: Action = 1, Reward = 1.00\n",
      "Step 168: Action = 0, Reward = 1.00\n",
      "Step 169: Action = 1, Reward = 1.00\n",
      "Step 170: Action = 1, Reward = 1.00\n",
      "Step 171: Action = 0, Reward = 1.00\n",
      "Step 172: Action = 0, Reward = 1.00\n",
      "Step 173: Action = 1, Reward = 1.00\n",
      "Step 174: Action = 1, Reward = 1.00\n",
      "Step 175: Action = 0, Reward = 1.00\n",
      "Step 176: Action = 1, Reward = 1.00\n",
      "Step 177: Action = 0, Reward = 1.00\n",
      "Step 178: Action = 0, Reward = 1.00\n",
      "Step 179: Action = 1, Reward = 1.00\n",
      "Step 180: Action = 0, Reward = 1.00\n",
      "Step 181: Action = 1, Reward = 1.00\n",
      "Step 182: Action = 1, Reward = 1.00\n",
      "Step 183: Action = 0, Reward = 1.00\n",
      "Step 184: Action = 0, Reward = 1.00\n",
      "Step 185: Action = 1, Reward = 1.00\n",
      "Step 186: Action = 1, Reward = 1.00\n",
      "Step 187: Action = 0, Reward = 1.00\n",
      "Step 188: Action = 1, Reward = 1.00\n",
      "Step 189: Action = 0, Reward = 1.00\n",
      "Step 190: Action = 0, Reward = 1.00\n",
      "Step 191: Action = 1, Reward = 1.00\n",
      "Step 192: Action = 0, Reward = 1.00\n",
      "Step 193: Action = 1, Reward = 1.00\n",
      "Step 194: Action = 1, Reward = 1.00\n",
      "Step 195: Action = 0, Reward = 1.00\n",
      "Step 196: Action = 0, Reward = 1.00\n",
      "Step 197: Action = 1, Reward = 1.00\n",
      "Step 198: Action = 1, Reward = 1.00\n",
      "Step 199: Action = 0, Reward = 1.00\n",
      "Step 200: Action = 0, Reward = 1.00\n",
      "Step 201: Action = 1, Reward = 1.00\n",
      "Step 202: Action = 1, Reward = 1.00\n",
      "Step 203: Action = 0, Reward = 1.00\n",
      "Step 204: Action = 1, Reward = 1.00\n",
      "Step 205: Action = 0, Reward = 1.00\n",
      "Step 206: Action = 0, Reward = 1.00\n",
      "Step 207: Action = 1, Reward = 1.00\n",
      "Step 208: Action = 0, Reward = 1.00\n",
      "Step 209: Action = 1, Reward = 1.00\n",
      "Step 210: Action = 1, Reward = 1.00\n",
      "Step 211: Action = 0, Reward = 1.00\n",
      "Step 212: Action = 0, Reward = 1.00\n",
      "Step 213: Action = 1, Reward = 1.00\n",
      "Step 214: Action = 1, Reward = 1.00\n",
      "Step 215: Action = 0, Reward = 1.00\n",
      "Step 216: Action = 0, Reward = 1.00\n",
      "Step 217: Action = 1, Reward = 1.00\n",
      "Step 218: Action = 1, Reward = 1.00\n",
      "Step 219: Action = 0, Reward = 1.00\n",
      "Step 220: Action = 1, Reward = 1.00\n",
      "Step 221: Action = 0, Reward = 1.00\n",
      "Step 222: Action = 0, Reward = 1.00\n",
      "Step 223: Action = 1, Reward = 1.00\n",
      "Step 224: Action = 0, Reward = 1.00\n",
      "Step 225: Action = 1, Reward = 1.00\n",
      "Step 226: Action = 1, Reward = 1.00\n",
      "Step 227: Action = 0, Reward = 1.00\n",
      "Step 228: Action = 0, Reward = 1.00\n",
      "Step 229: Action = 1, Reward = 1.00\n",
      "Step 230: Action = 1, Reward = 1.00\n",
      "Step 231: Action = 0, Reward = 1.00\n",
      "Step 232: Action = 0, Reward = 1.00\n",
      "Step 233: Action = 1, Reward = 1.00\n",
      "Step 234: Action = 1, Reward = 1.00\n",
      "Step 235: Action = 0, Reward = 1.00\n",
      "Step 236: Action = 1, Reward = 1.00\n",
      "Step 237: Action = 0, Reward = 1.00\n",
      "Step 238: Action = 0, Reward = 1.00\n",
      "Step 239: Action = 1, Reward = 1.00\n",
      "Step 240: Action = 0, Reward = 1.00\n",
      "Step 241: Action = 1, Reward = 1.00\n",
      "Step 242: Action = 1, Reward = 1.00\n",
      "Step 243: Action = 0, Reward = 1.00\n",
      "Step 244: Action = 0, Reward = 1.00\n",
      "Step 245: Action = 1, Reward = 1.00\n",
      "Step 246: Action = 1, Reward = 1.00\n",
      "Step 247: Action = 0, Reward = 1.00\n",
      "Step 248: Action = 1, Reward = 1.00\n",
      "Step 249: Action = 0, Reward = 1.00\n",
      "Step 250: Action = 0, Reward = 1.00\n",
      "Step 251: Action = 1, Reward = 1.00\n",
      "Step 252: Action = 0, Reward = 1.00\n",
      "Step 253: Action = 1, Reward = 1.00\n",
      "Step 254: Action = 1, Reward = 1.00\n",
      "Step 255: Action = 0, Reward = 1.00\n",
      "Step 256: Action = 0, Reward = 1.00\n",
      "Step 257: Action = 1, Reward = 1.00\n",
      "Step 258: Action = 1, Reward = 1.00\n",
      "Step 259: Action = 0, Reward = 1.00\n",
      "Step 260: Action = 0, Reward = 1.00\n",
      "Step 261: Action = 1, Reward = 1.00\n",
      "Step 262: Action = 1, Reward = 1.00\n",
      "Step 263: Action = 0, Reward = 1.00\n",
      "Step 264: Action = 0, Reward = 1.00\n",
      "Step 265: Action = 1, Reward = 1.00\n",
      "Step 266: Action = 1, Reward = 1.00\n",
      "Step 267: Action = 0, Reward = 1.00\n",
      "Step 268: Action = 1, Reward = 1.00\n",
      "Step 269: Action = 0, Reward = 1.00\n",
      "Step 270: Action = 0, Reward = 1.00\n",
      "Step 271: Action = 1, Reward = 1.00\n",
      "Step 272: Action = 0, Reward = 1.00\n",
      "Step 273: Action = 1, Reward = 1.00\n",
      "Step 274: Action = 1, Reward = 1.00\n",
      "Step 275: Action = 0, Reward = 1.00\n",
      "Step 276: Action = 0, Reward = 1.00\n",
      "Step 277: Action = 1, Reward = 1.00\n",
      "Step 278: Action = 1, Reward = 1.00\n",
      "Step 279: Action = 0, Reward = 1.00\n",
      "Step 280: Action = 0, Reward = 1.00\n",
      "Step 281: Action = 1, Reward = 1.00\n",
      "Step 282: Action = 1, Reward = 1.00\n",
      "Step 283: Action = 0, Reward = 1.00\n",
      "Step 284: Action = 0, Reward = 1.00\n",
      "Step 285: Action = 1, Reward = 1.00\n",
      "Step 286: Action = 1, Reward = 1.00\n",
      "Step 287: Action = 0, Reward = 1.00\n",
      "Step 288: Action = 0, Reward = 1.00\n",
      "Step 289: Action = 1, Reward = 1.00\n",
      "Step 290: Action = 1, Reward = 1.00\n",
      "Step 291: Action = 0, Reward = 1.00\n",
      "Step 292: Action = 0, Reward = 1.00\n",
      "Step 293: Action = 1, Reward = 1.00\n",
      "Step 294: Action = 1, Reward = 1.00\n",
      "Step 295: Action = 0, Reward = 1.00\n",
      "Step 296: Action = 1, Reward = 1.00\n",
      "Step 297: Action = 0, Reward = 1.00\n",
      "Step 298: Action = 0, Reward = 1.00\n",
      "Step 299: Action = 1, Reward = 1.00\n",
      "Step 300: Action = 0, Reward = 1.00\n",
      "Step 301: Action = 1, Reward = 1.00\n",
      "Step 302: Action = 1, Reward = 1.00\n",
      "Step 303: Action = 0, Reward = 1.00\n",
      "Step 304: Action = 0, Reward = 1.00\n",
      "Step 305: Action = 1, Reward = 1.00\n",
      "Step 306: Action = 1, Reward = 1.00\n",
      "Step 307: Action = 0, Reward = 1.00\n",
      "Step 308: Action = 0, Reward = 1.00\n",
      "Step 309: Action = 1, Reward = 1.00\n",
      "Step 310: Action = 1, Reward = 1.00\n",
      "Step 311: Action = 0, Reward = 1.00\n",
      "Step 312: Action = 0, Reward = 1.00\n",
      "Step 313: Action = 1, Reward = 1.00\n",
      "Step 314: Action = 1, Reward = 1.00\n",
      "Step 315: Action = 0, Reward = 1.00\n",
      "Step 316: Action = 0, Reward = 1.00\n",
      "Step 317: Action = 1, Reward = 1.00\n",
      "Step 318: Action = 1, Reward = 1.00\n",
      "Step 319: Action = 0, Reward = 1.00\n",
      "Step 320: Action = 1, Reward = 1.00\n",
      "Step 321: Action = 0, Reward = 1.00\n",
      "Step 322: Action = 0, Reward = 1.00\n",
      "Step 323: Action = 1, Reward = 1.00\n",
      "Step 324: Action = 0, Reward = 1.00\n",
      "Step 325: Action = 1, Reward = 1.00\n",
      "Step 326: Action = 1, Reward = 1.00\n",
      "Step 327: Action = 0, Reward = 1.00\n",
      "Step 328: Action = 0, Reward = 1.00\n",
      "Step 329: Action = 1, Reward = 1.00\n",
      "Step 330: Action = 1, Reward = 1.00\n",
      "Step 331: Action = 0, Reward = 1.00\n",
      "Step 332: Action = 0, Reward = 1.00\n",
      "Step 333: Action = 1, Reward = 1.00\n",
      "Step 334: Action = 1, Reward = 1.00\n",
      "Step 335: Action = 0, Reward = 1.00\n",
      "Step 336: Action = 1, Reward = 1.00\n",
      "Step 337: Action = 0, Reward = 1.00\n",
      "Step 338: Action = 0, Reward = 1.00\n",
      "Step 339: Action = 1, Reward = 1.00\n",
      "Step 340: Action = 0, Reward = 1.00\n",
      "Step 341: Action = 1, Reward = 1.00\n",
      "Step 342: Action = 1, Reward = 1.00\n",
      "Step 343: Action = 0, Reward = 1.00\n",
      "Step 344: Action = 0, Reward = 1.00\n",
      "Step 345: Action = 1, Reward = 1.00\n",
      "Step 346: Action = 1, Reward = 1.00\n",
      "Step 347: Action = 0, Reward = 1.00\n",
      "Step 348: Action = 0, Reward = 1.00\n",
      "Step 349: Action = 1, Reward = 1.00\n",
      "Step 350: Action = 1, Reward = 1.00\n",
      "Step 351: Action = 0, Reward = 1.00\n",
      "Step 352: Action = 1, Reward = 1.00\n",
      "Step 353: Action = 0, Reward = 1.00\n",
      "Step 354: Action = 0, Reward = 1.00\n",
      "Step 355: Action = 1, Reward = 1.00\n",
      "Step 356: Action = 0, Reward = 1.00\n",
      "Step 357: Action = 1, Reward = 1.00\n",
      "Step 358: Action = 1, Reward = 1.00\n",
      "Step 359: Action = 0, Reward = 1.00\n",
      "Step 360: Action = 0, Reward = 1.00\n",
      "Step 361: Action = 1, Reward = 1.00\n",
      "Step 362: Action = 1, Reward = 1.00\n",
      "Step 363: Action = 0, Reward = 1.00\n",
      "Step 364: Action = 0, Reward = 1.00\n",
      "Step 365: Action = 1, Reward = 1.00\n",
      "Step 366: Action = 1, Reward = 1.00\n",
      "Step 367: Action = 0, Reward = 1.00\n",
      "Step 368: Action = 0, Reward = 1.00\n",
      "Step 369: Action = 1, Reward = 1.00\n",
      "Step 370: Action = 1, Reward = 1.00\n",
      "Step 371: Action = 0, Reward = 1.00\n",
      "Step 372: Action = 1, Reward = 1.00\n",
      "Step 373: Action = 0, Reward = 1.00\n",
      "Step 374: Action = 0, Reward = 1.00\n",
      "Step 375: Action = 1, Reward = 1.00\n",
      "Step 376: Action = 0, Reward = 1.00\n",
      "Step 377: Action = 1, Reward = 1.00\n",
      "Step 378: Action = 1, Reward = 1.00\n",
      "Step 379: Action = 0, Reward = 1.00\n",
      "Step 380: Action = 0, Reward = 1.00\n",
      "Step 381: Action = 1, Reward = 1.00\n",
      "Step 382: Action = 1, Reward = 1.00\n",
      "Step 383: Action = 0, Reward = 1.00\n",
      "Step 384: Action = 0, Reward = 1.00\n",
      "Step 385: Action = 1, Reward = 1.00\n",
      "Step 386: Action = 1, Reward = 1.00\n",
      "Step 387: Action = 0, Reward = 1.00\n",
      "Step 388: Action = 0, Reward = 1.00\n",
      "Step 389: Action = 1, Reward = 1.00\n",
      "Step 390: Action = 1, Reward = 1.00\n",
      "Step 391: Action = 0, Reward = 1.00\n",
      "Step 392: Action = 0, Reward = 1.00\n",
      "Step 393: Action = 1, Reward = 1.00\n",
      "Step 394: Action = 1, Reward = 1.00\n",
      "Step 395: Action = 0, Reward = 1.00\n",
      "Step 396: Action = 0, Reward = 1.00\n",
      "Step 397: Action = 1, Reward = 1.00\n",
      "Step 398: Action = 1, Reward = 1.00\n",
      "Step 399: Action = 0, Reward = 1.00\n",
      "Step 400: Action = 0, Reward = 1.00\n",
      "Step 401: Action = 1, Reward = 1.00\n",
      "Step 402: Action = 1, Reward = 1.00\n",
      "Step 403: Action = 0, Reward = 1.00\n",
      "Step 404: Action = 0, Reward = 1.00\n",
      "Step 405: Action = 1, Reward = 1.00\n",
      "Step 406: Action = 1, Reward = 1.00\n",
      "Step 407: Action = 0, Reward = 1.00\n",
      "Step 408: Action = 0, Reward = 1.00\n",
      "Step 409: Action = 1, Reward = 1.00\n",
      "Step 410: Action = 1, Reward = 1.00\n",
      "Step 411: Action = 0, Reward = 1.00\n",
      "Step 412: Action = 0, Reward = 1.00\n",
      "Step 413: Action = 1, Reward = 1.00\n",
      "Step 414: Action = 1, Reward = 1.00\n",
      "Step 415: Action = 0, Reward = 1.00\n",
      "Step 416: Action = 0, Reward = 1.00\n",
      "Step 417: Action = 1, Reward = 1.00\n",
      "Step 418: Action = 1, Reward = 1.00\n",
      "Step 419: Action = 0, Reward = 1.00\n",
      "Step 420: Action = 0, Reward = 1.00\n",
      "Step 421: Action = 1, Reward = 1.00\n",
      "Step 422: Action = 1, Reward = 1.00\n",
      "Step 423: Action = 0, Reward = 1.00\n",
      "Step 424: Action = 0, Reward = 1.00\n",
      "Step 425: Action = 1, Reward = 1.00\n",
      "Step 426: Action = 1, Reward = 1.00\n",
      "Step 427: Action = 0, Reward = 1.00\n",
      "Step 428: Action = 0, Reward = 1.00\n",
      "Step 429: Action = 1, Reward = 1.00\n",
      "Step 430: Action = 1, Reward = 1.00\n",
      "Step 431: Action = 0, Reward = 1.00\n",
      "Step 432: Action = 0, Reward = 1.00\n",
      "Step 433: Action = 1, Reward = 1.00\n",
      "Step 434: Action = 1, Reward = 1.00\n",
      "Step 435: Action = 0, Reward = 1.00\n",
      "Step 436: Action = 0, Reward = 1.00\n",
      "Step 437: Action = 1, Reward = 1.00\n",
      "Step 438: Action = 1, Reward = 1.00\n",
      "Step 439: Action = 0, Reward = 1.00\n",
      "Step 440: Action = 0, Reward = 1.00\n",
      "Step 441: Action = 1, Reward = 1.00\n",
      "Step 442: Action = 0, Reward = 1.00\n",
      "Step 443: Action = 1, Reward = 1.00\n",
      "Step 444: Action = 1, Reward = 1.00\n",
      "Step 445: Action = 0, Reward = 1.00\n",
      "Step 446: Action = 0, Reward = 1.00\n",
      "Step 447: Action = 1, Reward = 1.00\n",
      "Step 448: Action = 1, Reward = 1.00\n",
      "Step 449: Action = 0, Reward = 1.00\n",
      "Step 450: Action = 1, Reward = 1.00\n",
      "Step 451: Action = 0, Reward = 1.00\n",
      "Step 452: Action = 0, Reward = 1.00\n",
      "Step 453: Action = 1, Reward = 1.00\n",
      "Step 454: Action = 0, Reward = 1.00\n",
      "Step 455: Action = 1, Reward = 1.00\n",
      "Step 456: Action = 1, Reward = 1.00\n",
      "Step 457: Action = 0, Reward = 1.00\n",
      "Step 458: Action = 0, Reward = 1.00\n",
      "Step 459: Action = 1, Reward = 1.00\n",
      "Step 460: Action = 1, Reward = 1.00\n",
      "Step 461: Action = 0, Reward = 1.00\n",
      "Step 462: Action = 0, Reward = 1.00\n",
      "Step 463: Action = 1, Reward = 1.00\n",
      "Step 464: Action = 1, Reward = 1.00\n",
      "Step 465: Action = 0, Reward = 1.00\n",
      "Step 466: Action = 0, Reward = 1.00\n",
      "Step 467: Action = 1, Reward = 1.00\n",
      "Step 468: Action = 1, Reward = 1.00\n",
      "Step 469: Action = 0, Reward = 1.00\n",
      "Step 470: Action = 0, Reward = 1.00\n",
      "Step 471: Action = 1, Reward = 1.00\n",
      "Step 472: Action = 1, Reward = 1.00\n",
      "Step 473: Action = 0, Reward = 1.00\n",
      "Step 474: Action = 0, Reward = 1.00\n",
      "Step 475: Action = 1, Reward = 1.00\n",
      "Step 476: Action = 1, Reward = 1.00\n",
      "Step 477: Action = 0, Reward = 1.00\n",
      "Step 478: Action = 0, Reward = 1.00\n",
      "Step 479: Action = 1, Reward = 1.00\n",
      "Step 480: Action = 1, Reward = 1.00\n",
      "Step 481: Action = 0, Reward = 1.00\n",
      "Step 482: Action = 0, Reward = 1.00\n",
      "Step 483: Action = 1, Reward = 1.00\n",
      "Step 484: Action = 1, Reward = 1.00\n",
      "Step 485: Action = 0, Reward = 1.00\n",
      "Step 486: Action = 0, Reward = 1.00\n",
      "Step 487: Action = 1, Reward = 1.00\n",
      "Step 488: Action = 1, Reward = 1.00\n",
      "Step 489: Action = 0, Reward = 1.00\n",
      "Step 490: Action = 0, Reward = 1.00\n",
      "Step 491: Action = 1, Reward = 1.00\n",
      "Step 492: Action = 1, Reward = 1.00\n",
      "Step 493: Action = 0, Reward = 1.00\n",
      "Step 494: Action = 0, Reward = 1.00\n",
      "Step 495: Action = 1, Reward = 1.00\n",
      "Step 496: Action = 1, Reward = 1.00\n",
      "Step 497: Action = 0, Reward = 1.00\n",
      "Step 498: Action = 0, Reward = 1.00\n",
      "Step 499: Action = 1, Reward = 1.00\n",
      "Step 500: Action = 1, Reward = 1.00\n",
      "Step 501: Action = 0, Reward = 1.00\n",
      "Step 502: Action = 0, Reward = 1.00\n",
      "Step 503: Action = 1, Reward = 1.00\n",
      "Step 504: Action = 1, Reward = 1.00\n",
      "Step 505: Action = 0, Reward = 1.00\n",
      "Step 506: Action = 0, Reward = 1.00\n",
      "Step 507: Action = 1, Reward = 1.00\n",
      "Step 508: Action = 1, Reward = 1.00\n",
      "Step 509: Action = 0, Reward = 1.00\n",
      "Step 510: Action = 0, Reward = 1.00\n",
      "Step 511: Action = 1, Reward = 1.00\n",
      "Step 512: Action = 1, Reward = 1.00\n",
      "Step 513: Action = 0, Reward = 1.00\n",
      "Step 514: Action = 0, Reward = 1.00\n",
      "Step 515: Action = 1, Reward = 1.00\n",
      "Step 516: Action = 1, Reward = 1.00\n",
      "Step 517: Action = 0, Reward = 1.00\n",
      "Step 518: Action = 0, Reward = 1.00\n",
      "Step 519: Action = 1, Reward = 1.00\n",
      "Step 520: Action = 1, Reward = 1.00\n",
      "Step 521: Action = 0, Reward = 1.00\n",
      "Step 522: Action = 0, Reward = 1.00\n",
      "Step 523: Action = 1, Reward = 1.00\n",
      "Step 524: Action = 1, Reward = 1.00\n",
      "Step 525: Action = 0, Reward = 1.00\n",
      "Step 526: Action = 0, Reward = 1.00\n",
      "Step 527: Action = 1, Reward = 1.00\n",
      "Step 528: Action = 1, Reward = 1.00\n",
      "Step 529: Action = 0, Reward = 1.00\n",
      "Step 530: Action = 0, Reward = 1.00\n",
      "Step 531: Action = 1, Reward = 1.00\n",
      "Step 532: Action = 1, Reward = 1.00\n",
      "Step 533: Action = 0, Reward = 1.00\n",
      "Step 534: Action = 0, Reward = 1.00\n",
      "Step 535: Action = 1, Reward = 1.00\n",
      "Step 536: Action = 0, Reward = 1.00\n",
      "Step 537: Action = 1, Reward = 1.00\n",
      "Step 538: Action = 1, Reward = 1.00\n",
      "Step 539: Action = 0, Reward = 1.00\n",
      "Step 540: Action = 0, Reward = 1.00\n",
      "Step 541: Action = 1, Reward = 1.00\n",
      "Step 542: Action = 1, Reward = 1.00\n",
      "Step 543: Action = 0, Reward = 1.00\n",
      "Step 544: Action = 0, Reward = 1.00\n",
      "Step 545: Action = 1, Reward = 1.00\n",
      "Step 546: Action = 1, Reward = 1.00\n",
      "Step 547: Action = 0, Reward = 1.00\n",
      "Step 548: Action = 1, Reward = 1.00\n",
      "Step 549: Action = 0, Reward = 1.00\n",
      "Step 550: Action = 0, Reward = 1.00\n",
      "Step 551: Action = 1, Reward = 1.00\n",
      "Step 552: Action = 0, Reward = 1.00\n",
      "Step 553: Action = 1, Reward = 1.00\n",
      "Step 554: Action = 1, Reward = 1.00\n",
      "Step 555: Action = 0, Reward = 1.00\n",
      "Step 556: Action = 0, Reward = 1.00\n",
      "Step 557: Action = 1, Reward = 1.00\n",
      "Step 558: Action = 1, Reward = 1.00\n",
      "Step 559: Action = 0, Reward = 1.00\n",
      "Step 560: Action = 0, Reward = 1.00\n",
      "Step 561: Action = 1, Reward = 1.00\n",
      "Step 562: Action = 1, Reward = 1.00\n",
      "Step 563: Action = 0, Reward = 1.00\n",
      "Step 564: Action = 0, Reward = 1.00\n",
      "Step 565: Action = 1, Reward = 1.00\n",
      "Step 566: Action = 1, Reward = 1.00\n",
      "Step 567: Action = 0, Reward = 1.00\n",
      "Step 568: Action = 0, Reward = 1.00\n",
      "Step 569: Action = 1, Reward = 1.00\n",
      "Step 570: Action = 1, Reward = 1.00\n",
      "Step 571: Action = 0, Reward = 1.00\n",
      "Step 572: Action = 0, Reward = 1.00\n",
      "Step 573: Action = 1, Reward = 1.00\n",
      "Step 574: Action = 1, Reward = 1.00\n",
      "Step 575: Action = 0, Reward = 1.00\n",
      "Step 576: Action = 0, Reward = 1.00\n",
      "Step 577: Action = 1, Reward = 1.00\n",
      "Step 578: Action = 1, Reward = 1.00\n",
      "Step 579: Action = 0, Reward = 1.00\n",
      "Step 580: Action = 0, Reward = 1.00\n",
      "Step 581: Action = 1, Reward = 1.00\n",
      "Step 582: Action = 1, Reward = 1.00\n",
      "Step 583: Action = 0, Reward = 1.00\n",
      "Step 584: Action = 0, Reward = 1.00\n",
      "Step 585: Action = 1, Reward = 1.00\n",
      "Step 586: Action = 0, Reward = 1.00\n",
      "Step 587: Action = 1, Reward = 1.00\n",
      "Step 588: Action = 1, Reward = 1.00\n",
      "Step 589: Action = 0, Reward = 1.00\n",
      "Step 590: Action = 1, Reward = 1.00\n",
      "Step 591: Action = 0, Reward = 1.00\n",
      "Step 592: Action = 0, Reward = 1.00\n",
      "Step 593: Action = 1, Reward = 1.00\n",
      "Step 594: Action = 0, Reward = 1.00\n",
      "Step 595: Action = 1, Reward = 1.00\n",
      "Step 596: Action = 1, Reward = 1.00\n",
      "Step 597: Action = 0, Reward = 1.00\n",
      "Step 598: Action = 0, Reward = 1.00\n",
      "Step 599: Action = 1, Reward = 1.00\n",
      "Step 600: Action = 1, Reward = 1.00\n",
      "Step 601: Action = 0, Reward = 1.00\n",
      "Step 602: Action = 0, Reward = 1.00\n",
      "Step 603: Action = 1, Reward = 1.00\n",
      "Step 604: Action = 1, Reward = 1.00\n",
      "Step 605: Action = 0, Reward = 1.00\n",
      "Step 606: Action = 0, Reward = 1.00\n",
      "Step 607: Action = 1, Reward = 1.00\n",
      "Step 608: Action = 1, Reward = 1.00\n",
      "Step 609: Action = 0, Reward = 1.00\n",
      "Step 610: Action = 0, Reward = 1.00\n",
      "Step 611: Action = 1, Reward = 1.00\n",
      "Step 612: Action = 1, Reward = 1.00\n",
      "Step 613: Action = 0, Reward = 1.00\n",
      "Step 614: Action = 0, Reward = 1.00\n",
      "Step 615: Action = 1, Reward = 1.00\n",
      "Step 616: Action = 1, Reward = 1.00\n",
      "Step 617: Action = 0, Reward = 1.00\n",
      "Step 618: Action = 0, Reward = 1.00\n",
      "Step 619: Action = 1, Reward = 1.00\n",
      "Step 620: Action = 1, Reward = 1.00\n",
      "Step 621: Action = 0, Reward = 1.00\n",
      "Step 622: Action = 0, Reward = 1.00\n",
      "Step 623: Action = 1, Reward = 1.00\n",
      "Step 624: Action = 1, Reward = 1.00\n",
      "Step 625: Action = 0, Reward = 1.00\n",
      "Step 626: Action = 0, Reward = 1.00\n",
      "Step 627: Action = 1, Reward = 1.00\n",
      "Step 628: Action = 0, Reward = 1.00\n",
      "Step 629: Action = 1, Reward = 1.00\n",
      "Step 630: Action = 1, Reward = 1.00\n",
      "Step 631: Action = 0, Reward = 1.00\n",
      "Step 632: Action = 0, Reward = 1.00\n",
      "Step 633: Action = 1, Reward = 1.00\n",
      "Step 634: Action = 1, Reward = 1.00\n",
      "Step 635: Action = 0, Reward = 1.00\n",
      "Step 636: Action = 0, Reward = 1.00\n",
      "Step 637: Action = 1, Reward = 1.00\n",
      "Step 638: Action = 1, Reward = 1.00\n",
      "Step 639: Action = 0, Reward = 1.00\n",
      "Step 640: Action = 0, Reward = 1.00\n",
      "Step 641: Action = 1, Reward = 1.00\n",
      "Step 642: Action = 1, Reward = 1.00\n",
      "Step 643: Action = 0, Reward = 1.00\n",
      "Step 644: Action = 0, Reward = 1.00\n",
      "Step 645: Action = 1, Reward = 1.00\n",
      "Step 646: Action = 1, Reward = 1.00\n",
      "Step 647: Action = 0, Reward = 1.00\n",
      "Step 648: Action = 0, Reward = 1.00\n",
      "Step 649: Action = 1, Reward = 1.00\n",
      "Step 650: Action = 1, Reward = 1.00\n",
      "Step 651: Action = 0, Reward = 1.00\n",
      "Step 652: Action = 0, Reward = 1.00\n",
      "Step 653: Action = 1, Reward = 1.00\n",
      "Step 654: Action = 1, Reward = 1.00\n",
      "Step 655: Action = 0, Reward = 1.00\n",
      "Step 656: Action = 0, Reward = 1.00\n",
      "Step 657: Action = 1, Reward = 1.00\n",
      "Step 658: Action = 1, Reward = 1.00\n",
      "Step 659: Action = 0, Reward = 1.00\n",
      "Step 660: Action = 0, Reward = 1.00\n",
      "Step 661: Action = 1, Reward = 1.00\n",
      "Step 662: Action = 1, Reward = 1.00\n",
      "Step 663: Action = 0, Reward = 1.00\n",
      "Step 664: Action = 0, Reward = 1.00\n",
      "Step 665: Action = 1, Reward = 1.00\n",
      "Step 666: Action = 1, Reward = 1.00\n",
      "Step 667: Action = 0, Reward = 1.00\n",
      "Step 668: Action = 0, Reward = 1.00\n",
      "Step 669: Action = 1, Reward = 1.00\n",
      "Step 670: Action = 1, Reward = 1.00\n",
      "Step 671: Action = 0, Reward = 1.00\n",
      "Step 672: Action = 0, Reward = 1.00\n",
      "Step 673: Action = 1, Reward = 1.00\n",
      "Step 674: Action = 1, Reward = 1.00\n",
      "Step 675: Action = 0, Reward = 1.00\n",
      "Step 676: Action = 0, Reward = 1.00\n",
      "Step 677: Action = 1, Reward = 1.00\n",
      "Step 678: Action = 1, Reward = 1.00\n",
      "Step 679: Action = 0, Reward = 1.00\n",
      "Step 680: Action = 1, Reward = 1.00\n",
      "Step 681: Action = 0, Reward = 1.00\n",
      "Step 682: Action = 0, Reward = 1.00\n",
      "Step 683: Action = 1, Reward = 1.00\n",
      "Step 684: Action = 0, Reward = 1.00\n",
      "Step 685: Action = 1, Reward = 1.00\n",
      "Step 686: Action = 1, Reward = 1.00\n",
      "Step 687: Action = 0, Reward = 1.00\n",
      "Step 688: Action = 0, Reward = 1.00\n",
      "Step 689: Action = 1, Reward = 1.00\n",
      "Step 690: Action = 1, Reward = 1.00\n",
      "Step 691: Action = 0, Reward = 1.00\n",
      "Step 692: Action = 0, Reward = 1.00\n",
      "Step 693: Action = 1, Reward = 1.00\n",
      "Step 694: Action = 1, Reward = 1.00\n",
      "Step 695: Action = 0, Reward = 1.00\n",
      "Step 696: Action = 0, Reward = 1.00\n",
      "Step 697: Action = 1, Reward = 1.00\n",
      "Step 698: Action = 1, Reward = 1.00\n",
      "Step 699: Action = 0, Reward = 1.00\n",
      "Step 700: Action = 0, Reward = 1.00\n",
      "Step 701: Action = 1, Reward = 1.00\n",
      "Step 702: Action = 1, Reward = 1.00\n",
      "Step 703: Action = 0, Reward = 1.00\n",
      "Step 704: Action = 0, Reward = 1.00\n",
      "Step 705: Action = 1, Reward = 1.00\n",
      "Step 706: Action = 0, Reward = 1.00\n",
      "Step 707: Action = 1, Reward = 1.00\n",
      "Step 708: Action = 1, Reward = 1.00\n",
      "Step 709: Action = 0, Reward = 1.00\n",
      "Step 710: Action = 0, Reward = 1.00\n",
      "Step 711: Action = 1, Reward = 1.00\n",
      "Step 712: Action = 1, Reward = 1.00\n",
      "Step 713: Action = 0, Reward = 1.00\n",
      "Step 714: Action = 0, Reward = 1.00\n",
      "Step 715: Action = 1, Reward = 1.00\n",
      "Step 716: Action = 1, Reward = 1.00\n",
      "Step 717: Action = 0, Reward = 1.00\n",
      "Step 718: Action = 0, Reward = 1.00\n",
      "Step 719: Action = 1, Reward = 1.00\n",
      "Step 720: Action = 1, Reward = 1.00\n",
      "Step 721: Action = 0, Reward = 1.00\n",
      "Step 722: Action = 0, Reward = 1.00\n",
      "Step 723: Action = 1, Reward = 1.00\n",
      "Step 724: Action = 1, Reward = 1.00\n",
      "Step 725: Action = 0, Reward = 1.00\n",
      "Step 726: Action = 1, Reward = 1.00\n",
      "Step 727: Action = 0, Reward = 1.00\n",
      "Step 728: Action = 0, Reward = 1.00\n",
      "Step 729: Action = 1, Reward = 1.00\n",
      "Step 730: Action = 0, Reward = 1.00\n",
      "Step 731: Action = 1, Reward = 1.00\n",
      "Step 732: Action = 1, Reward = 1.00\n",
      "Step 733: Action = 0, Reward = 1.00\n",
      "Step 734: Action = 0, Reward = 1.00\n",
      "Step 735: Action = 1, Reward = 1.00\n",
      "Step 736: Action = 1, Reward = 1.00\n",
      "Step 737: Action = 0, Reward = 1.00\n",
      "Step 738: Action = 0, Reward = 1.00\n",
      "Step 739: Action = 1, Reward = 1.00\n",
      "Step 740: Action = 1, Reward = 1.00\n",
      "Step 741: Action = 0, Reward = 1.00\n",
      "Step 742: Action = 0, Reward = 1.00\n",
      "Step 743: Action = 1, Reward = 1.00\n",
      "Step 744: Action = 1, Reward = 1.00\n",
      "Step 745: Action = 0, Reward = 1.00\n",
      "Step 746: Action = 0, Reward = 1.00\n",
      "Step 747: Action = 1, Reward = 1.00\n",
      "Step 748: Action = 1, Reward = 1.00\n",
      "Step 749: Action = 0, Reward = 1.00\n",
      "Step 750: Action = 0, Reward = 1.00\n",
      "Step 751: Action = 1, Reward = 1.00\n",
      "Step 752: Action = 0, Reward = 1.00\n",
      "Step 753: Action = 1, Reward = 1.00\n",
      "Step 754: Action = 1, Reward = 1.00\n",
      "Step 755: Action = 0, Reward = 1.00\n",
      "Step 756: Action = 0, Reward = 1.00\n",
      "Step 757: Action = 1, Reward = 1.00\n",
      "Step 758: Action = 1, Reward = 1.00\n",
      "Step 759: Action = 0, Reward = 1.00\n",
      "Step 760: Action = 0, Reward = 1.00\n",
      "Step 761: Action = 1, Reward = 1.00\n",
      "Step 762: Action = 1, Reward = 1.00\n",
      "Step 763: Action = 0, Reward = 1.00\n",
      "Step 764: Action = 0, Reward = 1.00\n",
      "Step 765: Action = 1, Reward = 1.00\n",
      "Step 766: Action = 1, Reward = 1.00\n",
      "Step 767: Action = 0, Reward = 1.00\n",
      "Step 768: Action = 0, Reward = 1.00\n",
      "Step 769: Action = 1, Reward = 1.00\n",
      "Step 770: Action = 1, Reward = 1.00\n",
      "Step 771: Action = 0, Reward = 1.00\n",
      "Step 772: Action = 1, Reward = 1.00\n",
      "Step 773: Action = 0, Reward = 1.00\n",
      "Step 774: Action = 0, Reward = 1.00\n",
      "Step 775: Action = 1, Reward = 1.00\n",
      "Step 776: Action = 0, Reward = 1.00\n",
      "Step 777: Action = 1, Reward = 1.00\n",
      "Step 778: Action = 1, Reward = 1.00\n",
      "Step 779: Action = 0, Reward = 1.00\n",
      "Step 780: Action = 0, Reward = 1.00\n",
      "Step 781: Action = 1, Reward = 1.00\n",
      "Step 782: Action = 1, Reward = 1.00\n",
      "Step 783: Action = 0, Reward = 1.00\n",
      "Step 784: Action = 0, Reward = 1.00\n",
      "Step 785: Action = 1, Reward = 1.00\n",
      "Step 786: Action = 1, Reward = 1.00\n",
      "Step 787: Action = 0, Reward = 1.00\n",
      "Step 788: Action = 0, Reward = 1.00\n",
      "Step 789: Action = 1, Reward = 1.00\n",
      "Step 790: Action = 1, Reward = 1.00\n",
      "Step 791: Action = 0, Reward = 1.00\n",
      "Step 792: Action = 0, Reward = 1.00\n",
      "Step 793: Action = 1, Reward = 1.00\n",
      "Step 794: Action = 1, Reward = 1.00\n",
      "Step 795: Action = 0, Reward = 1.00\n",
      "Step 796: Action = 0, Reward = 1.00\n",
      "Step 797: Action = 1, Reward = 1.00\n",
      "Step 798: Action = 0, Reward = 1.00\n",
      "Step 799: Action = 1, Reward = 1.00\n",
      "Step 800: Action = 1, Reward = 1.00\n",
      "Step 801: Action = 0, Reward = 1.00\n",
      "Step 802: Action = 0, Reward = 1.00\n",
      "Step 803: Action = 1, Reward = 1.00\n",
      "Step 804: Action = 1, Reward = 1.00\n",
      "Step 805: Action = 0, Reward = 1.00\n",
      "Step 806: Action = 0, Reward = 1.00\n",
      "Step 807: Action = 1, Reward = 1.00\n",
      "Step 808: Action = 1, Reward = 1.00\n",
      "Step 809: Action = 0, Reward = 1.00\n",
      "Step 810: Action = 0, Reward = 1.00\n",
      "Step 811: Action = 1, Reward = 1.00\n",
      "Step 812: Action = 1, Reward = 1.00\n",
      "Step 813: Action = 0, Reward = 1.00\n",
      "Step 814: Action = 0, Reward = 1.00\n",
      "Step 815: Action = 1, Reward = 1.00\n",
      "Step 816: Action = 1, Reward = 1.00\n",
      "Step 817: Action = 0, Reward = 1.00\n",
      "Step 818: Action = 0, Reward = 1.00\n",
      "Step 819: Action = 1, Reward = 1.00\n",
      "Step 820: Action = 1, Reward = 1.00\n",
      "Step 821: Action = 0, Reward = 1.00\n",
      "Step 822: Action = 0, Reward = 1.00\n",
      "Step 823: Action = 1, Reward = 1.00\n",
      "Step 824: Action = 1, Reward = 1.00\n",
      "Step 825: Action = 0, Reward = 1.00\n",
      "Step 826: Action = 0, Reward = 1.00\n",
      "Step 827: Action = 1, Reward = 1.00\n",
      "Step 828: Action = 1, Reward = 1.00\n",
      "Step 829: Action = 0, Reward = 1.00\n",
      "Step 830: Action = 0, Reward = 1.00\n",
      "Step 831: Action = 1, Reward = 1.00\n",
      "Step 832: Action = 0, Reward = 1.00\n",
      "Step 833: Action = 1, Reward = 1.00\n",
      "Step 834: Action = 1, Reward = 1.00\n",
      "Step 835: Action = 0, Reward = 1.00\n",
      "Step 836: Action = 0, Reward = 1.00\n",
      "Step 837: Action = 1, Reward = 1.00\n",
      "Step 838: Action = 1, Reward = 1.00\n",
      "Step 839: Action = 0, Reward = 1.00\n",
      "Step 840: Action = 0, Reward = 1.00\n",
      "Step 841: Action = 1, Reward = 1.00\n",
      "Step 842: Action = 1, Reward = 1.00\n",
      "Step 843: Action = 0, Reward = 1.00\n",
      "Step 844: Action = 0, Reward = 1.00\n",
      "Step 845: Action = 1, Reward = 1.00\n",
      "Step 846: Action = 1, Reward = 1.00\n",
      "Step 847: Action = 0, Reward = 1.00\n",
      "Step 848: Action = 0, Reward = 1.00\n",
      "Step 849: Action = 1, Reward = 1.00\n",
      "Step 850: Action = 1, Reward = 1.00\n",
      "Step 851: Action = 0, Reward = 1.00\n",
      "Step 852: Action = 0, Reward = 1.00\n",
      "Step 853: Action = 1, Reward = 1.00\n",
      "Step 854: Action = 1, Reward = 1.00\n",
      "Step 855: Action = 0, Reward = 1.00\n",
      "Step 856: Action = 0, Reward = 1.00\n",
      "Step 857: Action = 1, Reward = 1.00\n",
      "Step 858: Action = 1, Reward = 1.00\n",
      "Step 859: Action = 0, Reward = 1.00\n",
      "Step 860: Action = 0, Reward = 1.00\n",
      "Step 861: Action = 1, Reward = 1.00\n",
      "Step 862: Action = 1, Reward = 1.00\n",
      "Step 863: Action = 0, Reward = 1.00\n",
      "Step 864: Action = 0, Reward = 1.00\n",
      "Step 865: Action = 1, Reward = 1.00\n",
      "Step 866: Action = 1, Reward = 1.00\n",
      "Step 867: Action = 0, Reward = 1.00\n",
      "Step 868: Action = 0, Reward = 1.00\n",
      "Step 869: Action = 1, Reward = 1.00\n",
      "Step 870: Action = 1, Reward = 1.00\n",
      "Step 871: Action = 0, Reward = 1.00\n",
      "Step 872: Action = 0, Reward = 1.00\n",
      "Step 873: Action = 1, Reward = 1.00\n",
      "Step 874: Action = 1, Reward = 1.00\n",
      "Step 875: Action = 0, Reward = 1.00\n",
      "Step 876: Action = 0, Reward = 1.00\n",
      "Step 877: Action = 1, Reward = 1.00\n",
      "Step 878: Action = 1, Reward = 1.00\n",
      "Step 879: Action = 0, Reward = 1.00\n",
      "Step 880: Action = 0, Reward = 1.00\n",
      "Step 881: Action = 1, Reward = 1.00\n",
      "Step 882: Action = 0, Reward = 1.00\n",
      "Step 883: Action = 1, Reward = 1.00\n",
      "Step 884: Action = 1, Reward = 1.00\n",
      "Step 885: Action = 0, Reward = 1.00\n",
      "Step 886: Action = 0, Reward = 1.00\n",
      "Step 887: Action = 1, Reward = 1.00\n",
      "Step 888: Action = 1, Reward = 1.00\n",
      "Step 889: Action = 0, Reward = 1.00\n",
      "Step 890: Action = 0, Reward = 1.00\n",
      "Step 891: Action = 1, Reward = 1.00\n",
      "Step 892: Action = 1, Reward = 1.00\n",
      "Step 893: Action = 0, Reward = 1.00\n",
      "Step 894: Action = 0, Reward = 1.00\n",
      "Step 895: Action = 1, Reward = 1.00\n",
      "Step 896: Action = 1, Reward = 1.00\n",
      "Step 897: Action = 0, Reward = 1.00\n",
      "Step 898: Action = 0, Reward = 1.00\n",
      "Step 899: Action = 1, Reward = 1.00\n",
      "Step 900: Action = 1, Reward = 1.00\n",
      "Step 901: Action = 0, Reward = 1.00\n",
      "Step 902: Action = 0, Reward = 1.00\n",
      "Step 903: Action = 1, Reward = 1.00\n",
      "Step 904: Action = 1, Reward = 1.00\n",
      "Step 905: Action = 0, Reward = 1.00\n",
      "Step 906: Action = 0, Reward = 1.00\n",
      "Step 907: Action = 1, Reward = 1.00\n",
      "Step 908: Action = 1, Reward = 1.00\n",
      "Step 909: Action = 0, Reward = 1.00\n",
      "Step 910: Action = 0, Reward = 1.00\n",
      "Step 911: Action = 1, Reward = 1.00\n",
      "Step 912: Action = 0, Reward = 1.00\n",
      "Step 913: Action = 1, Reward = 1.00\n",
      "Step 914: Action = 1, Reward = 1.00\n",
      "Step 915: Action = 0, Reward = 1.00\n",
      "Step 916: Action = 1, Reward = 1.00\n",
      "Step 917: Action = 0, Reward = 1.00\n",
      "Step 918: Action = 0, Reward = 1.00\n",
      "Step 919: Action = 1, Reward = 1.00\n",
      "Step 920: Action = 0, Reward = 1.00\n",
      "Step 921: Action = 1, Reward = 1.00\n",
      "Step 922: Action = 1, Reward = 1.00\n",
      "Step 923: Action = 0, Reward = 1.00\n",
      "Step 924: Action = 0, Reward = 1.00\n",
      "Step 925: Action = 1, Reward = 1.00\n",
      "Step 926: Action = 1, Reward = 1.00\n",
      "Step 927: Action = 0, Reward = 1.00\n",
      "Step 928: Action = 0, Reward = 1.00\n",
      "Step 929: Action = 1, Reward = 1.00\n",
      "Step 930: Action = 1, Reward = 1.00\n",
      "Step 931: Action = 0, Reward = 1.00\n",
      "Step 932: Action = 0, Reward = 1.00\n",
      "Step 933: Action = 1, Reward = 1.00\n",
      "Step 934: Action = 1, Reward = 1.00\n",
      "Step 935: Action = 0, Reward = 1.00\n",
      "Step 936: Action = 0, Reward = 1.00\n",
      "Step 937: Action = 1, Reward = 1.00\n",
      "Step 938: Action = 0, Reward = 1.00\n",
      "Step 939: Action = 1, Reward = 1.00\n",
      "Step 940: Action = 1, Reward = 1.00\n",
      "Step 941: Action = 0, Reward = 1.00\n",
      "Step 942: Action = 0, Reward = 1.00\n",
      "Step 943: Action = 1, Reward = 1.00\n",
      "Step 944: Action = 1, Reward = 1.00\n",
      "Step 945: Action = 0, Reward = 1.00\n",
      "Step 946: Action = 0, Reward = 1.00\n",
      "Step 947: Action = 1, Reward = 1.00\n",
      "Step 948: Action = 1, Reward = 1.00\n",
      "Step 949: Action = 0, Reward = 1.00\n",
      "Step 950: Action = 0, Reward = 1.00\n",
      "Step 951: Action = 1, Reward = 1.00\n",
      "Step 952: Action = 1, Reward = 1.00\n",
      "Step 953: Action = 0, Reward = 1.00\n",
      "Step 954: Action = 0, Reward = 1.00\n",
      "Step 955: Action = 1, Reward = 1.00\n",
      "Step 956: Action = 1, Reward = 1.00\n",
      "Step 957: Action = 0, Reward = 1.00\n",
      "Step 958: Action = 0, Reward = 1.00\n",
      "Step 959: Action = 1, Reward = 1.00\n",
      "Step 960: Action = 1, Reward = 1.00\n",
      "Step 961: Action = 0, Reward = 1.00\n",
      "Step 962: Action = 0, Reward = 1.00\n",
      "Step 963: Action = 1, Reward = 1.00\n",
      "Step 964: Action = 1, Reward = 1.00\n",
      "Step 965: Action = 0, Reward = 1.00\n",
      "Step 966: Action = 0, Reward = 1.00\n",
      "Step 967: Action = 1, Reward = 1.00\n",
      "Step 968: Action = 1, Reward = 1.00\n",
      "Step 969: Action = 0, Reward = 1.00\n",
      "Step 970: Action = 0, Reward = 1.00\n",
      "Step 971: Action = 1, Reward = 1.00\n",
      "Step 972: Action = 1, Reward = 1.00\n",
      "Step 973: Action = 0, Reward = 1.00\n",
      "Step 974: Action = 0, Reward = 1.00\n",
      "Step 975: Action = 1, Reward = 1.00\n",
      "Step 976: Action = 1, Reward = 1.00\n",
      "Step 977: Action = 0, Reward = 1.00\n",
      "Step 978: Action = 0, Reward = 1.00\n",
      "Step 979: Action = 1, Reward = 1.00\n",
      "Step 980: Action = 0, Reward = 1.00\n",
      "Step 981: Action = 1, Reward = 1.00\n",
      "Step 982: Action = 1, Reward = 1.00\n",
      "Step 983: Action = 0, Reward = 1.00\n",
      "Step 984: Action = 0, Reward = 1.00\n",
      "Step 985: Action = 1, Reward = 1.00\n",
      "Step 986: Action = 1, Reward = 1.00\n",
      "Step 987: Action = 0, Reward = 1.00\n",
      "Step 988: Action = 1, Reward = 1.00\n",
      "Step 989: Action = 0, Reward = 1.00\n",
      "Step 990: Action = 0, Reward = 1.00\n",
      "Step 991: Action = 1, Reward = 1.00\n",
      "Step 992: Action = 0, Reward = 1.00\n",
      "Step 993: Action = 1, Reward = 1.00\n",
      "Step 994: Action = 1, Reward = 1.00\n",
      "Step 995: Action = 0, Reward = 1.00\n",
      "Step 996: Action = 0, Reward = 1.00\n",
      "Step 997: Action = 1, Reward = 1.00\n",
      "Step 998: Action = 1, Reward = 1.00\n",
      "Step 999: Action = 0, Reward = 1.00\n",
      "Episode 1: Total Reward = 1000.00\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# ----- Load class ActorCritic và PPO của bạn trước đây -----\n",
    "# from PPO import PPO\n",
    "\n",
    "# ---------- Hyperparameters ----------\n",
    "env_name = \"CartPole-v1\"\n",
    "has_continuous_action_space = False\n",
    "\n",
    "env = gym.make(env_name, render_mode=\"human\")  # Dùng render_mode để xem animation\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# ---------- Khởi tạo PPO agent ----------\n",
    "ppo_agent = PPO(state_dim, action_dim,\n",
    "                lr_actor=0.0003,\n",
    "                lr_critic=0.001,\n",
    "                gamma=0.99,\n",
    "                K_epochs=40,\n",
    "                eps_clip=0.2,\n",
    "                has_continuous_action_space=has_continuous_action_space)\n",
    "\n",
    "# ---------- Load mô hình đã huấn luyện ----------\n",
    "ppo_agent.policy.load_state_dict(torch.load('ppo_best_model.pth'))\n",
    "ppo_agent.policy.eval()\n",
    "\n",
    "# ---------- Hàm lấy hành động deterministic ----------\n",
    "def get_deterministic_action(state):\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        action_probs = ppo_agent.policy.actor(state)\n",
    "        action = torch.argmax(action_probs, dim=1).item()\n",
    "    return action\n",
    "\n",
    "# ---------- Đánh giá mô hình ----------\n",
    "num_eval_episodes = 1\n",
    "\n",
    "for ep in range(num_eval_episodes):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "\n",
    "    step = 0\n",
    "    while not done:\n",
    "        action = get_deterministic_action(state)\n",
    "        next_state, reward,done,_,_ = env.step(action)\n",
    "\n",
    "        print(f\"Step {step}: Action = {action}, Reward = {reward:.2f}\")\n",
    "        ep_reward += reward\n",
    "        state = next_state\n",
    "        step += 1\n",
    "        if step >= 1000:\n",
    "            break\n",
    "\n",
    "        time.sleep(0.02)  # Để thấy rõ animation\n",
    "\n",
    "    print(f\"Episode {ep + 1}: Total Reward = {ep_reward:.2f}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
